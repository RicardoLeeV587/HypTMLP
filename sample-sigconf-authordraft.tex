%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%\documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf,anonymous,review]{acmart}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Manifold Constrained Tabular Deep Neural Networks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract} 


Despite the success of Tabular Deep Neural Network (DNN) models, Gradient-Boosted Decision Tree (GBDT) models remain the dominant baseline for tabular data, largely due to their efficiency in processing heterogeneous features and hierarchical reasoning. The critical bottleneck in tabular DNNs is representation granularity mismatch and inability to compensate for hierarchical reasoning. %, in which categorical features are modeled with fine-grained \textit{value-level} embeddings, while numerical features are reduced to coarse \textit{feature-level} linear projections. 
% Moreover, Euclidean spaces struggle to model the exponential branching of decision trees because they lack the necessary intrinsic geometry. 
To bridge this gap, we propose a novel \textit{Hyperbolic Decision Embedding (HDE)} framework that simulates a geometric isomorphism between feature embeddings and decision-tree inference. HDE unifies feature embedding by treating both categorical values and numerical ranges as \textit{Latent Decision Nodes (LDNs)} within the Poincaré ball hyperbolic representation. Specifically, we introduce \textit{Soft Decision Routing} to semantically discretize numerical features, aligning them with the value-level granularity of categorical embeddings. By leveraging the exponential growth in hyperbolic geometry, HDE implicitly enforces hierarchical constraints, mimicking tree-like decision boundaries in the continuous-valued space. Furthermore, we incorporate an \textit{entropy-aware} mechanism to adaptively allocate semantic capacity for numerical LDNs. Our model, \textit{HDE-Net}, achieves state-of-the-art performance on the TALENT-tiny-core classification benchmark, comprising 30 datasets. HDE-Net emerged \textit{ranks 1st} among both tree-based and tabular DNN baselines.

%Despite the prevalence of Deep Neural Networks (DNNs), Gradient Boosted Decision Trees (GBDTs) remain the dominant baseline for tabular data, largely due to their unified handling of heterogeneous features and efficient hierarchical reasoning. By comparing their modeling processes, we identify a critical bottleneck in existing DNNs: a \textbf{Representation Granularity Mismatch}, where categorical features are modeled via fine-grained \textit{value-level} embeddings, while numerical features are reduced to coarse \textit{feature-level} linear projections. Moreover, Euclidean spaces struggle to model the exponential branching of decision trees, lacking the necessary intrinsic geometry. To bridge this gap, we propose \textbf{Hyperbolic Decision Embedding (HDE)}, a framework that establishes a geometric isomorphism between neural embeddings and decision tree inference. HDE unifies feature representation by treating both categorical values and numerical ranges as \textbf{Latent Decision Nodes (LDNs)} within the Poincaré ball. Specifically, we introduce \textbf{Soft Decision Routing} to semantically discretize numerical features, aligning them with the value-level granularity of categorical embeddings. By leveraging the exponential volume growth of hyperbolic geometry, HDE implicitly enforces hierarchical constraints, mimicking tree-like decision boundaries within the continuous valued space. Furthermore, we incorporate an \textbf{entropy-aware} mechanism to adaptively allocate semantic capacity for numerical LDNs. This \textit{``heavy-representation, light-backbone''} design enables our full model, \textbf{HDE-Net}, to achieve State-of-the-Art performance using only a simple MLP predictor. Extensive experiments on the TALENT-tiny-core classification benchmark demonstrate that HDE-Net ranks \textbf{1st (4.17)}, significantly outperforming both leading tree-based and deep learning baselines.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Use, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

Tabular data remains the cornerstone of critical real-world applications, from financial risk assessment to healthcare diagnostics \cite{guo2017deepfm, buczak2015survey, hyland2020early, liu2025talent}. Unlike perceptual data such as images or text, tabular data is characterized by heterogeneous features, dense semantics, and a lack of spatial or sequential invariance. While Deep Neural Networks (DNNs) have revolutionized computer vision and NLP, their dominance has not fully transferred to the tabular domain \cite{yan2023t2g, ye2024revisiting}. Recent extensive benchmarks \cite{liu2025talent} and studies \cite{ye2024revisiting, ye2025closer} consistently demonstrate that Gradient Boosted Decision Trees (GBDTs), such as XGBoost \cite{chen2016xgboost} and CatBoost \cite{prokhorenkova2018catboost}, remain the dominant baseline in terms of both accuracy and efficiency.

Why do GBDTs still maintain this edge? We argue that their success hinges on two fundamental principles: \textbf{1. Unified Node Representation}: Despite the heterogeneity of raw features, tree models abstract both categorical values and continuous numerical ranges into a unified formalism: discrete tree nodes. This unification prevents the model from being biased towards specific feature types, ensuring robust performance across diverse tabular scenarios. \textbf{2. Hierarchical Reasoning}: The tree-like reasoning mechanism naturally captures non-linear decision boundaries and high-order interactions through hierarchical paths.

In contrast, existing DNNs suffer from a \textit{Representation Granularity Mismatch}. While categorical features are naturally modeled at the fine-grained ``value-level'' (via lookup tables for each unique value), numerical features are typically modeled at the coarse ``feature-level'' (via linear projection for the entire column). This mismatch restricts numerical embeddings to a linear value space, failing to capture the non-linear semantics inherent in data, and thereby forcing subsequent layers to compensate for this semantic deficiency. Furthermore, Euclidean space lacks the intrinsic geometry to model the exponential branching of decision trees, leading to a \textit{Capacity Mismatch}.

To address these issues, prior works have diverged into four compromise paths:
\begin{itemize}
    \item Ad-hoc Fusion (e.g., TabTransformer \cite{huang2020tabtransformer}, RealMLP \cite{holzmuller2025realmlp}, DCNv2 \cite{wang2021dcn}): Treats features differently and fuses them via heuristic adjustments, failing to resolve the underlying semantic misalignment.
    \item Pseudo-Alignment (e.g., FT-Transformer \cite{gorishniy2021revisiting}, T2G-Former \cite{yan2023t2g}): Forces dimensional alignment via tokenizers but ignores the granularity mismatch. This often necessitates bloated backbones (e.g., Transformers \cite{vaswani2017attention}) or rigid graph priors to compensate for the lack of hierarchical expressiveness in the embedding layer.
    \item Retrieval-based (e.g., ModernNCA \cite{ye2024revisiting}, TabR \cite{gorishniy2023tabr}): Sidestep the representation challenge by retrieving neighbors. While effective, this incurs high inference costs and sensitivity to Out-of-Distribution (OOD) shifts.
    \item Foundation Models (e.g., TabPFN \cite{hollmann2022tabpfn}): Relies on massive pre-training. While powerful, this approach faces scalability limits regarding context length on large-scale datasets.
\end{itemize}

In this work, we propose to bridge this gap not by patching DNN architectures, but by establishing a geometric isomorphism with the inference logic of decision trees. We introduce \textbf{Hyperbolic Decision Embedding (HDE)}, a framework that achieves true value-level unification for heterogeneous features within a hyperbolic manifold: the Poincaré ball \cite{nickel2017poincare, becigneul2018riemannian, peng2021hyperbolic}.

Our approach rests on two key innovations. First, to align numerical features with the value-level granularity of categorical embeddings, we introduce \textbf{Soft Decision Routing}. This mechanism dynamically directs continuous values to  \textbf{Latent Decision Nodes (LDNs)}, effectively simulating the \textit{split and tree nodes construct} operations of a Decision Tree and upgrading numerical representations from feature-level to value-level, thus unifying the semantics granularity of both feature types. Second, by embedding these nodes in \textbf{hyperbolic space}, we leverage its exponential volume growth to implicitly enforce the hierarchical constraints inherent in tabular data. Additionally, we incorporate an \textbf{Entropy-aware} mechanism to adaptively allocate semantic capacity (number of LDNs) for each numerical feature.

Guided by the design philosophy of ``heavy-representation, light-backbone'', we construct \textbf{HDE-Net}, which couples the expressive HDE layer with a simple MLP predictor. Our main contributions are:
\begin{itemize}
    \item \textbf{Tree-Isomorphic Manifold Representation}: We propose a new paradigm that semantically discretizes numerical features into Hyperbolic Latent Decision Nodes, resolving the representation granularity mismatch in existing DNNs.
    \item \textbf{Implicit Hierarchical Constraint}: We leverage the intrinsic curvature of hyperbolic space to enforce hierarchical constraints, enhancing robustness without explicit graphs or retrieval.
    \item \textbf{Adaptive Semantic Capacity}: We introduce an entropy-aware allocation strategy to balance numerical LDNs' expressiveness and efficiency, preventing over- and under-parameterization.
    \item \textbf{SOTA Performance \& Efficiency}: HDE-Net achieves state-of-the-art results on the TALENT-tiny-core classification benchmark (Rank 1st), surpassing CatBoost, RealMLP, and ModernNCA. % Moreover, it demonstrates superior extensibility as a backbone for multi-modal healthcare tasks.
\end{itemize}

% Table generated by Excel2LaTeX from sheet 'Sheet6'
\begin{table}[htbp]
\small
  \centering
  \caption{Performance breakdown on the TALENT-tiny classification benchmark across dataset subsets defined by feature composition. Values represent the Average Rank (lower is better). Numbers in parentheses indicate the rank shift relative to the overall (``All'') performance (\textcolor{green}{green/-}: ranking down; \textcolor{red}{red/+}: ranking up).}
    \begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{Models} & All   & Num-only & Num > Cat & Num < Cat \\
    \midrule
    \multicolumn{1}{c}{\multirow{3}[2]{*}{\makecell*[c]{Tree\\Models}}} & Xgboost & 7.4  & 10.5 \textcolor{green}{(+3.1)} & 5.9 \textcolor{red}{(-1.5)} & 3.0 \textcolor{red}{(-4.3)} \\
          & LightGBM & 8.4  & 11.0 \textcolor{green}{(+2.6)} & 6.5 \textcolor{red}{(-1.9)} & 5.5 \textcolor{red}{(-2.8)} \\
          & CatBoost & 8.6  & 12.3 \textcolor{green}{(+3.7)} & 6.4 \textcolor{red}{(-2.2)} & 3.9 \textcolor{red}{(-4.7)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Ad-hoc\\Fusion}}} & RealMLP & 8.9  & 6.4 \textcolor{red}{(-2.5)} & 9.7 \textcolor{green}{(+0.8)} & 12.9 \textcolor{green}{(+4.0)} \\
          & MLP   & 14.9 & 10.1 \textcolor{red}{(-4.8)} & 17.8 \textcolor{green}{(+2.8)} & 21.0 \textcolor{green}{(+6.1)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Pseudo\\Alignment}}} & FTT   & 11.6 & 12.3 \textcolor{green}{(+0.6)} & 11.3 \textcolor{red}{(-0.3)} & 10.7 \textcolor{red}{(-0.9)} \\
          & ExcelFormer & 13.9 & 14.7 \textcolor{green}{(+0.8)} & 13.3 \textcolor{red}{(-0.6)} & 13.2 \textcolor{red}{(-0.7)} \\
    \bottomrule
    \end{tabular}%
  \label{tab:motivation}%
\end{table}%

\section{Related Work}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{Arch.png} 
  \caption {\textbf{Geometric Isomorphism between Decision Trees and Hyperbolic Decision Embedding (HDE).} 
\textbf{(Left)} Example DT for mood disorder prediction. 
\textbf{(Middle)} Latent Decision Nodes (LDNs) embedded in the Poincaré ball: categorical values (e.g., \textcolor{red}{Male}, \textcolor{teal}{Female}) map directly to LDNs, while numerical features (e.g., Age) are softly routed to multiple LDNs to represent multiple ranges split just like DT.
\textbf{(Right)} HDE-Net inference: hyperbolic embeddings (\textit{solid}) are mapped to the Euclidean tangent space (\textit{dashed}) via $\log_{\mathbf{0}}(\cdot)$ for efficient weighted aggregation (just for numerical features) and MLP classification.}
  \label{fig:arch}
\end{figure*}

\subsection{Deep Tabular Learning: Paradigms and Empirical Limitations}
\label{sec:rel_tabular}

Recent advances in Deep Tabular Learning can be categorized into four paradigms. We exclude Retrieval-based \cite{ye2024revisiting} and Foundation Models \cite{hollmann2022tabpfn} from this discussion to focus on intrinsic architecture design under comparable computational budgets. The remaining approaches primarily follow two paths: (1) \textbf{Ad-hoc Fusion}, which applies distinct processing pipelines to different feature types. For instance, RealMLP \cite{holzmuller2025realmlp} employs Periodic-Bias-Linear-Dense (PBLD) embeddings \cite{gorishniy2022embeddings} for numerical features and simple one-hot encoding for categorical ones, fused via residual blocks. TabTransformer \cite{huang2020tabtransformer} processes categorical features via Transformers while concatenating raw numerical features directly. (2) \textbf{Pseudo-Alignment}, which forces dimensional alignment via tokenizers. The representative FT-Transformer \cite{gorishniy2021revisiting} projects numerical features via linear layers and looks up categorical embeddings, processing them uniformly through a deep Transformer backbone. AutoInt\cite{song2019autoint}, ExcelFormer\cite{chen2023excelformer}, and T2G-Former \cite{yan2023t2g} use similar mechanism to unify feature representation.

To rigorously analyze their efficacy, we conduct a performance breakdown on the TALENT benchmark (see Table \ref{tab:motivation}) across datasets grouped by feature composition. For clarity and ease of presentation, we only shown the top-performing models on the TALENT benchmark. The results reveal the characteristics of different models: \textbf{(i) GBDT Dominance:} Tree models remain the superior baseline across most scenarios, except for purely numerical tasks. \textbf{(ii) The Specialization Trap of Ad-hoc Fusion:} While RealMLP achieves SOTA performance on \textit{Num-only} datasets, its performance collapses on \textit{Categorical-dominant} domains. This volatility indicates a failure to unify heterogeneous information within a coherent reasoning framework. \textbf{(iii) The Inefficiency of Pseudo-Alignment:} FT-Transformer demonstrates stability across feature types, validating the utility of alignment. However, its inferior performance on numerical tasks compared to RealMLP confirms that simple linear projections are inadequate for numerical representation. Furthermore, despite utilizing a significantly heavier backbone, it fails to surpass simpler MLP-based architectures, highlighting a poor trade-off between model complexity and expressiveness.

\subsection{Hyperbolic Geometry in Representation Learning}
\label{sec:rel_hyperbolic}

Hyperbolic space is mathematically isomorphic to a continuous tree due to its exponential volume growth \cite{nickel2017poincare}. This property has driven a lot of representation learning in domains with explicit hierarchical structures, such as Graph Learning \cite{chami2019hyperbolic, dai2021hyperbolic}, Computer Vision \cite{lensink2022fully}, and Multimodal Learning (e.g., MERU \cite{desai2023hyperbolic}). General-purpose neural components, such as Hyperbolic Linear Layer \cite{shimizu2020hyperbolic}, have also been proposed to facilitate these applications.

Closer to our work, recent studies have extended traditional machine learning algorithms to non-Euclidean geometries. Notably, HyperDT \cite{chlenski2023fast} generalizes decision trees to hyperbolic space, validating the geometric alignment between tree-based splitting and hyperbolic manifolds. However, these methods operate as traditional classifiers; they lack the \textbf{end-to-end representation learning} capabilities of DNNs and cannot seamlessly integrate other unstructured modalities.

The application of hyperbolic geometry to \textbf{Tabular Deep Learning}, particularly for feature representation, remains a significant blind spot. HDE bridges this gap. By mapping features to Latent Decision Nodes in the Poincaré ball, we empower a simple MLP backbone with the hierarchical reasoning logic of decision trees, combining geometric interpretability with deep representation power.

\section{Preliminaries}
\label{sec:preliminaries}

\textbf{Problem Formulation.} 
We consider a supervised tabular learning task with a dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$. Each input instance $\mathbf{x}$ consists of  heterogeneous parts: numerical features $\mathbf{x}^{\text{num}} \in \mathbb{R}^{N_{num}}$ and categorical features $\mathbf{x}^{\text{cat}} \in \mathbb{R}^{N_{cat}}$. Our goal is to learn a function $f(\mathbf{x})$ that predicts the target $y$. As discussed, we aim to unify the representation of these heterogeneous features into a shared geometric space.

\noindent\textbf{The Poincaré Ball Model.} 
Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature. We utilize the $d$ dimensional Poincaré ball model $(\mathbb{B}_c^d, g^{\mathbb{B}})$, defined as the manifold $\mathbb{B}_c^d = \{ \mathbf{x} \in \mathbb{R}^d : c\| \mathbf{x} \|^2 < 1 \}$, where $c$ is a hyperparameter controlling the curvature (learnable). The induced distance between two points $\mathbf{x}, \mathbf{y} \in \mathbb{B}_c^d$ is given by:
\begin{equation}
    d_{\mathbb{B}}(\mathbf{x}, \mathbf{y}) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c} \| -\mathbf{x} \oplus_c \mathbf{y} \|),
\end{equation}
where $\oplus_c$ denotes the Möbius addition \cite{ungar2008analytic}. A key property of this space is that its volume grows exponentially with the radius. This aligns perfectly with the exponential growth of nodes in a decision tree as depth increases, making it an ideal continuous proxy for hierarchical tree structures \cite{nickel2017poincare}.

\noindent\textbf{Model Choice: Poincaré vs. Lorentz.} 
While the Lorentz model is often preferred for numerical stability in complex GNNs \cite{yang2022hyperbolic}, HDE avoids complex manifold operations (e.g., inter-node addition) and frequent Euclidean-Hyperbolic projections. We therefore select the Poincaré ball for its \textbf{conformality} (angle-preserving property). This allows us to intuitively encode hierarchy via the norm (depth) and logical affinity via angles (sibling relations), achieving a direct geometric isomorphism with decision trees.

\noindent\textbf{Tangent Space Projection.} 
To integrate hyperbolic embeddings with Euclidean neural networks (i.e., the MLP backbone), we utilize the \textbf{Logarithmic Map} to project points from the manifold to the Euclidean tangent space at the origin $\mathcal{T}_{\mathbf{0}}\mathbb{B}_c^d$:
\begin{equation}
    \log_{\mathbf{0}}(\mathbf{x}) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c}\|\mathbf{x}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|}.
\end{equation}
In HDE, we directly initialize Latent Decision Nodes on the manifold and use $\log_{\mathbf{0}}(\cdot)$ as the bridge to the Euclidean predictor.

\section{Methodology}
\label{sec:method}

% To bridge the representation gap between decision trees and neural networks, we propose \textbf{HDE-Net}, a hybrid architecture that integrates Hyperbolic Decision Embedding (HDE) with a lightweight MLP predictor. 
As illustrated in Figure \ref{fig:arch}, the HDE-Net framework consists of three key stages: (1) \textbf{Semantic Discretization}, where heterogeneous features are routed to a unified set of Latent Decision Nodes (LDNs); (2) \textbf{Hyperbolic Embedding}, where LDNs are optimized on the Poincaré ball to enforce implicit hierarchical constraints; and (3) \textbf{Tangent Projection \& Prediction}, where embeddings are projected to the Euclidean tangent space for efficient aggregation and classification.  Within the Semantic Discretization stage, we introduce two novel mechanisms specifically for numerical features: \textit{Entropy-Aware Capacity Allocation} to determine the number of LDNs, and \textit{Soft Decision Routing} to mimic tree-based interval discretization.

\subsection{Entropy-Aware Capacity Allocation}
\label{sec:entropy}
Assigning a fixed number of LDNs ($k$) to all numerical features leads to suboptimal resource allocation, simple features may suffer from over-parameterization, while complex ones face under-parameterization. To address this, we propose an \textbf{Entropy-aware Capacity Allocation} mechanism. We dynamically determine $k_j$ for each numerical feature $x_j^{num}$ based on its information entropy $H(x_j^{num})$ and effective sample size $N_{eff}$. This ensures that the model allocates more "decision bandwidth" to features with richer information, akin to how decision trees grow deeper for critical features. The detailed procedure is outlined in Algorithm \ref{alg:entropy}.

\subsection{Soft Decision Routing}
For numerical features, we simulate the splitting process of decision trees to achieve semantic discretization. Given a scalar input $x^{(num)}_j$, we employ a learnable gating mechanism, termed \textbf{Soft Decision Routing}, to compute a set of weights over $k_j$ LDNs. Specifically, we define the routing weights $\boldsymbol{\alpha}_j \in \mathbb{R}^{K_j}$ as:
\begin{equation}
    \boldsymbol{W}_j = \boldsymbol{a}_j x^{num}_j + \boldsymbol{b}_j, \quad \boldsymbol{W}_j \in \mathbb{R}^{k_j}
    \label{eq:router_weight}
\end{equation}
where $\boldsymbol{a}_j, \boldsymbol{b}_j$ are learnable parameters. Taking Figure~\ref{fig:arch} as an example, $k$ and $W$ are calculated for the "Age" column, and $j$ denotes the index of "Age" feature in numerical feature list. 
Crucially, we use the raw logits $\boldsymbol{w}_j$ for weighted aggregation without a softmax or sigmoid activation. This design allows for flexible intensity scaling, preserving the magnitude information of the input. It effectively performs a soft aggregation operation rather than a hard selection. For categorical features $x^{\text{cat}}_i$, the routing is deterministic, selecting the unique LDN corresponding to the category value. Consequently, categorical and numerical features are aligned in representational granularity, while numerical feature representations retain both linear (via intensity) and non-linear (via multiple LDNs) expressive capacity.

\begin{algorithm}[t]
\caption{Entropy-aware Capacity Allocation}
\label{alg:entropy}
\begin{algorithmic}[1]
\REQUIRE Dataset $\mathcal{D}$, Feature set $\mathcal{F}_{num}$, Max capacity $k_{max}$, Min capacity $k_{min}$
\ENSURE Capacity map $\mathcal{K} = \{k_j\}_{j \in \mathcal{F}_{num}}$
\FOR{each feature $x_j \in \mathcal{F}_{num}$}
    \STATE Calculate Shannon entropy $H(x_j)$ on non-missing values
    \STATE Calculate effective sample count $N_{eff}$
    \STATE Derive candidate capacity from entropy: $k_{ent} \leftarrow \max(k_{min}, \lfloor 2^{H(x_j)} \rfloor)$
    \STATE Derive candidate capacity from samples: $k_{spl} \leftarrow \max(k_{min}, \lfloor \log_2(N_{eff}) \rfloor)$
    \STATE Determine final capacity: $k_j \leftarrow \min(k_{max}, k_{ent}, k_{spl})$
    \STATE Store $k_j$ in $\mathcal{K}$
\ENDFOR
\RETURN $\mathcal{K}$
\end{algorithmic}
\end{algorithm}

\subsection{Latent Decision Nodes in Hyperbolic Space}
% We define the destinations of these routing paths as \textbf{Latent Decision Nodes (LDNs)}. 
Let $\mathcal{M} = \mathbb{B}_c^d$ be the $d$-dimensional Poincaré ball with curvature $c$. For all features (numerical and categorical), we initialize their LDNs directly on the same manifold $\mathcal{M}$:
\begin{equation}
    \mathbf{Z}_j = \{\mathbf{z}_{j,1}, \dots, \mathbf{z}_{j,k_j}\} \subset \mathbb{B}_c^d
\end{equation}
By optimizing these nodes in hyperbolic space, HDE implicitly enforces a hierarchical structure: general concepts (root-like) naturally cluster near the origin, while fine-grained values (leaf-like) are pushed towards the boundary. This geometric constraint enables the model to capture the hierarchical dependencies inherent in tabular data without explicit tree construction.

\subsection{Tangent Projection and Feature Aggregation}
To enable efficient inference and compatibility with standard neural layers, we decouple the geometric representation from the aggregation logic. Instead of performing expensive Möbius operations on the manifold, we project the retrieved LDNs to the Euclidean tangent space at the origin $\mathcal{T}_{\mathbf{0}}\mathbb{B}_c^d$ using the Logarithmic map:
\begin{equation}
    \mathbf{v}_{j,k} = \log_{\mathbf{0}}(\mathbf{z}_{j,k})
\end{equation}
Once projected, the rest computations are performed in the Euclidean space. For a numerical feature $x_j^{num}$, the final embedding $\mathbf{e}^{num}_j$ is the weighted aggregatation of its projected LDNs:
\begin{equation}
    \mathbf{e}^{num}_j = \frac{1}{k_j}\sum_{k=1}^{k_j} w_{j,k} \mathbf{v}_{j,k},  \quad \text{where } [w_{j,1}, \dots, w_{j, k_j}] =  \boldsymbol{W}_j
\end{equation}
For a categorical feature, $\mathbf{e}^{cat}_j$ is simply the projected vector of the selected LDN.

\subsection{Prediction and Hybrid Optimization}
The aggregated embeddings from all features are concatenated and fed into a standard MLP predictor. To train HDE-Net, we employ a \textbf{Hybrid Optimization} strategy. The LDN parameters $\mathbf{Z}$ residing on the manifold and the curvature $c$ are updated using Riemannian Adam \cite{becigneul2018riemannian}, which performs gradient updates along geodesics. All other parameters (Routing gates $\boldsymbol{a}, \boldsymbol{b}$ and MLP weights) are updated using standard AdamW in Euclidean space. This dual approach ensures that the learned representations strictly adhere to hyperbolic geometry while maintaining the computational efficiency of Euclidean inference.

\section{Experiments}
\label{sec:exp}

\subsection{Setups}
\label{sec:setups}

% Table generated by Excel2LaTeX from sheet 'Sheet3'\begin{table*}[htbp]
\small  \centering  \caption{Performance of HDE-Net across all datasets in the TALENT-tiny core classification benchmark}
  \scalebox{0.86}{    \begin{tabular}{lccccclccccc}    \toprule    \multicolumn{12}{c}{\textbf{HDE-Net}} \\    \midrule    \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} & \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} \\    \midrule    ada   & 80.90\% & 80.12\% & 80.47\% & 89.44\% & 85.60\% & law-school-admission & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% \\    airlines & 63.92\% & 63.26\% & 63.21\% & 65.05\% & 64.30\% & microaggregation2 & 57.55\% & 39.19\% & 41.22\% & 82.00\% & 63.83\% \\    allbp & 83.81\% & 64.80\% & 71.42\% & 94.95\% & 97.58\% & national-longitudinal & 99.81\% & 99.80\% & 99.81\% & 99.99\% & 99.82\% \\    ASP-POTASSCO & 37.71\% & 35.77\% & 36.03\% & 80.66\% & 42.59\% & okcupid\_stem & 67.98\% & 51.01\% & 52.81\% & 80.55\% & 75.35\% \\    autoUniv-au7-1100 & 42.35\% & 40.82\% & 40.16\% & 69.24\% & 41.95\% & online\_shoppers & 83.46\% & 77.78\% & 80.18\% & 91.79\% & 90.49\% \\    company\_bankruptcy & 85.15\% & 69.17\% & 74.33\% & 95.99\% & 97.52\% & ozone\_level & 49.11\% & 49.90\% & 49.50\% & 68.64\% & 98.03\% \\    eucalyptus & 73.34\% & 70.64\% & 71.42\% & 93.50\% & 73.92\% & pc4   & 82.58\% & 69.05\% & 73.08\% & 93.08\% & 90.72\% \\    Gender\_Gap & 53.54\% & 42.88\% & 43.90\% & 65.37\% & 60.17\% & PhishingWebsites & 97.91\% & 97.74\% & 97.82\% & 99.71\% & 97.85\% \\    hill-valley & 73.33\% & 72.81\% & 72.65\% & 77.48\% & 72.80\% & rice\_cammeo\_\&\_osmancik & 93.11\% & 92.33\% & 92.65\% & 97.62\% & 92.86\% \\    house\_16H & 88.40\% & 88.39\% & 88.39\% & 94.69\% & 88.39\% & shill-bidding & 64.99\% & 56.06\% & 56.99\% & 71.46\% & 90.24\% \\    ibm-employee-performance & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% & Shipping & 75.97\% & 73.07\% & 68.75\% & 73.77\% & 69.05\% \\    INNHotelsGroup & 87.26\% & 86.21\% & 86.69\% & 94.49\% & 88.43\% & statlog & 68.87\% & 67.75\% & 68.20\% & 73.80\% & 74.05\% \\    internet\_firewall & 93.22\% & 77.66\% & 81.90\% & 98.85\% & 93.09\% & thyroid & 96.34\% & 97.60\% & 96.92\% & 99.88\% & 99.40\% \\    jasmine & 82.10\% & 79.37\% & 78.90\% & 84.06\% & 79.35\% & waveform\_version\_1 & 87.10\% & 87.09\% & 87.06\% & 96.85\% & 87.12\% \\    jungle\_chess\_2pcs & 97.67\% & 97.68\% & 97.66\% & 99.88\% & 98.20\% & wine-quality-red & 39.23\% & 33.01\% & 33.67\% & 75.98\% & 62.97\% \\    \bottomrule    \end{tabular}%
    }  \label{tab:HDE-Net_perf}%\end{table*}%

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{overall_performance.png}
    \caption{Critical Difference (CD) diagram on the TALENT-tiny-core classification benchmark. Average ranks are calculated via the Wilcoxon-Holm test ($\alpha=0.05$). HDE-Net ranks 1st (4.1667), demonstrating a significant performance lead over 31 baselines.}
    \label{fig:cd_diagram}
\end{figure*}

\textbf{Datasets.} 
We evaluate HDE-Net on the TALENT-tiny core classification benchmark \cite{liu2025talent}, which comprises 30 datasets (see Appendix \ref{app:datasets} for details). Unlike traditional benchmarks that are often biased toward numerical-dominant data, TALENT-tiny core provides a balanced and fair representation of various feature compositions (Numerical-only, Numerical-dominant, and Categorical-dominant). This ensures that the evaluation is not skewed by any specific feature type. Detailed statistical information for each dataset is provided in Appendix \ref{app:datasets}.

\noindent\textbf{Baselines.}
We include a comprehensive pool of 31 baselines whose results are officially disclosed on the TALENT leaderboard. These include 10 \textit{Traditional Models} (e.g., CatBoost \cite{prokhorenkova2018catboost}, XGBoost \cite{chen2016xgboost}, LightGBM \cite{ke2017lightgbm}, Random Forest \cite{breiman2001random}, SVM \cite{hearst1998support}, KNN \cite{goldberger2004neighbourhood}) and 21 \textit{Deep Learning Models} (e.g., ModernNCA \cite{ye2024revisiting}, RealMLP \cite{holzmuller2024better}, FT-Transformer \cite{gorishniy2021revisiting}, TabNet \cite{arik2021tabnet}). A full list of these baselines is provided in Appendix \ref{app:comp_models}.

\noindent\textbf{Hyperparameter Settings.}
For HDE-Net, the dimension of Hyperbolic Decision Embeddings is fixed at 12 across all datasets. The number of Latent Decision Nodes (LDNs) for each numerical feature is adaptively assigned via the \textit{Entropy-Aware Capacity Allocation} mechanism (Sec. \ref{sec:entropy}) with $k_{\min}=2$ and $k_{\max}=8$. To ensure a fair comparison, the hyperparameters for the MLP backbone are retrieved from the TALENT repository for each specific dataset. For datasets without pre-tuned settings, we adopt the default configuration: two hidden layers of size 384 with a dropout of 0.1, trained with a learning rate of $3 \times 10^{-4}$ and weight decay of $1 \times 10^{-5}$. The LDNs are optimized using Riemannian Adam, with its learning rate and weight decay consistent with the standard AdamW optimizer used for Euclidean parameters.

\textbf{Evaluation Metrics and Ranking.}
All models were evaluated on all datasets with 10 repeated runs, and mean accuracy was recorded. We report the average rank computed using the Wilcoxon–Holm test \cite{demvsar2006statistical} over all accuracy results at a significance level of 0.05. It is critical to note that since ranking is a relative metric, the absolute rank of a model (e.g., HDE-Net) depends entirely on the specific model pool included in the comparison. Consequently, the ranks reported in the \textit{Overall Comparison} (Sec. \ref{sec:main_results}) and the \textit{Ablation Study} (Sec. \ref{sec:ablation}) may differ due to the varying sets of HDE variants involved.

\subsection{Main Performance Comparison}
\label{sec:main_results}

The overall performance of HDE-Net and 31 baseline models on the TALENT-tiny-core benchmark is shown in Table~\ref{tab:HDE-Net_perf}, and its ranking is summarized in Figure \ref{fig:cd_diagram}. HDE-Net achieves a leading Average Rank of \textbf{4.1667}, securing the \textbf{1st position} among all compared methods. 

As shown in the diagram, HDE-Net exhibits a clear statistical advantage over a vast majority of deep learning models and traditional tree-based models. Notably, HDE-Net significantly outperforms the current state-of-the-art retrieval-based model, ModernNCA (Rank 7.63), and the industrial gold standard, XGBoost (Rank 7.98). This result validates our core design philosophy: by establishing a geometric isomorphism with decision trees at the representation level, HDE-Net combines the robustness of tree models with the expressive power of neural networks, achieving superior generalization without the need for complex backbones or expensive retrieval processes.

\subsection{Robustness across Feature Scenarios}
\label{sec:robustness}

To further investigate the adaptability of HDE-Net to different feature compositions, we analyze the average ranking across three specific scenarios: \textit{Numerical-only} (Num-only), \textit{Numerical-dominant} (Num > Cat > 0), and \textit{Categorical-dominant} (Num < Cat). We select representative models from each paradigm for comparison: XGBoost (Tree), RealMLP (Ad-hoc Fusion), FT-Transformer (Pseudo-Alignment), and ModernNCA (Retrieval). The results are illustrated in Figure \ref{fig:scenarios}.

\noindent\textbf{Excellence in Numerical Representation.} In the \textit{Num-only} scenario, HDE-Net achieves a rank of 4.64, significantly outperforming RealMLP (7.04) and FT-Transformer (13.25). This gap proves that our Soft Decision Routing mechanism effectively captures the linear and non-linear semantics of numerical features, whereas the simple linear projections in FTT create a significant performance bottleneck.

\noindent\textbf{Overcoming the Specialization Trap.} The results highlight the extreme instability of Ad-hoc Fusion methods. RealMLP, while performing relatively well in Num-only (7.04), collapses in \textit{Num < Cat} scenarios (Rank 13.86). In contrast, HDE-Net remains highly robust, achieving a rank of 3.57 in categorical-heavy tasks, which is on par with XGBoost (3.43). This confirms that by constraining Latent Decision Nodes (LDNs) in the Poincaré ball, HDE-Net successfully replicates tree-like hierarchical reasoning, enabling the model to handle categorical dependencies as effectively as trees while maintaining the value-level numerical expressiveness.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{GroupedPerformance.png}
    \caption{Comparison of average rankings across different feature scenarios. HDE-Net demonstrates superior stability and performance across all scenarios, while methods like RealMLP exhibit severe specialization bias (collapsing in categorical-dominant tasks).}
    \label{fig:scenarios}
\end{figure}

\subsection{Mechanism Analysis and Ablation Study}
\label{sec:ablation}

In this section, we conduct a detailed ablation study to quantify the contributions of HDE-Net's core components and evaluate its sensitivity to parameters. \textit{Note on Rankings}: As average ranking is a relative metric, the absolute values reported in this section (e.g., in Figures \ref{fig:ablation_bar} and \ref{fig:sensitivity_bar}) differ from the main results in Section \ref{sec:main_results} because the model pool has been expanded to include HDE variants for specialized comparison.

\noindent\textbf{Impact of Core Mechanisms} 
To understand the necessity of each design choice, we evaluate two critical variants: \textit{HDE-noRouter}, which reverts numerical feature processing to a single hyperbolic linear mapping, and \textit{HDE-noHyp}, which retains the routing mechanism but constrains all LDNs to Euclidean space. We also include FT-Transformer as an external reference. As illustrated in Figure \ref{fig:ablation_bar}:

\begin{itemize}
	\item \textbf{The Primacy of Soft Decision Routing.} The most dramatic performance drop occurs in \textit{HDE-noRouter} (Rank 13.45). This confirms that resolving the representation granularity mismatch via Soft Decision Routing is the foundational prerequisite for deep tabular models. 
	\item \textbf{The SOTA-making Factor: Hyperbolic Constraints.} Introducing the hyperbolic manifold brings a significant jump from \textit{HDE-noHyp} (Rank 7.10) to \textit{HDE-Net} (Rank 4.93). This validates our claim that implicit hierarchical constraints are essential for achieving top-tier performance, especially in capturing tree-like reasoning patterns. 
	\item \textbf{Manifold Efficiency.} Notably, \textit{HDE-noRouter} (12-dim, MLP backbone) achieves parity with FT-Transformer (100+ dim, Transformer backbone), further confirms that HDE represent decision structures far more efficiently than complex Euclidean backbones.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{ablation.png}
    \caption{Ablation study of HDE-Net components. Comparisons show that Soft Decision Routing is the primary driver of performance, while Hyperbolic constraints provide the critical boost to secure SOTA status.}
    \label{fig:ablation_bar}
\end{figure}

\noindent\textbf{Efficacy of Entropy-aware Allocation}
We further analyze the parameter sensitivity of the number of LDNs ($k$). We compare HDE-Net (Adaptive $k$) against variants with fixed $k \in \{2, 4, 6, 8, 10\}$. As shown in Figure \ref{fig:sensitivity_bar}:

The results exhibit a clear U-shaped ranking trend for fixed-capacity models ($k=2$ to $k=10$). Models with $k \le 6$ suffer from under-parameterization (insufficient decision nodes), while $k=10$ shows signs of over-parameterization, increasing the complexity burden on the MLP classifier. 
Meanwhile, HDE-Net (Rank 5.73) significantly outperforms the best fixed variant ($k=8$, Rank 7.92). This empirical gap demonstrates the superiority of our adaptation which effectively assigns "decision bandwidth" where it is most needed, finding a superior balance between expressiveness and efficiency that no fixed-parameter model can achieve.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{K_sensitiveness.png}
    \caption{Parameter sensitivity analysis across different LDN capacities ($k$). HDE-Net (Adaptive $k$) consistently outperforms all fixed-$k$ variants, proving the necessity of entropy-aware allocation.}
    \label{fig:sensitivity_bar}
\end{figure}

\subsection{Qualitative Analysis: Geometric Isomorphism}
\label{sec:visualization}

To validate our core claim that HDE-Net establishes a geometric isomorphism with decision trees, we conduct a qualitative analysis focusing on split logic and hierarchical structure.

\noindent\textbf{Split Logic Alignment via Router Weights.} 
We examine whether the \textit{Soft Decision Router} reconstructs the range-splitting logic of decision trees. For a numerical feature $j$, we visualize the routing weights $w_{j,k}$ (from Eq.~\ref{eq:router_weight}) across its entire value range. As shown in Figure \ref{fig:router_vis}, the x-axis represents sorted feature values, and each colored line corresponds to the weight of a specific LDN. 
In ``ibm employee performance'' ($k=2$) and ``company bankruptcy'' ($k=4$) datasets, the intersection points of these weight curves (one LDN's influence begins to surpass another) align remarkably well with the hard split thresholds learned by a standard Decision Tree (via \textit{scikit-learn}). Moreover, the zero-crossing points (where $w_{j,k} = 0$) also align with signify a transition in the node's contribution from negative to positive, effectively acting as a soft decision boundary. This phenomenon demonstrates that HDE-Net implicitly reconstructs the discrete range-splitting mechanism of trees while maintaining a differentiable formulation suitable for gradient-based optimization.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{RouterVisual.png}
    \caption{Visualization of Soft Decision Router weights compared to Decision Tree split points. The intersections and zero-crossings of LDN weights closely match the tree's hard thresholds, proving the isomorphism in split logic.}
    \label{fig:router_vis}
\end{figure}

\noindent\textbf{Hierarchical Structure in Poincaré Space.}
We further visualize the learned distribution of LDNs to confirm their hierarchical nature. We first compute the pairwise manifold distance matrix $\mathbf{D} \in \mathbb{R}^{M \times M}$ for all $M$ active LDNs using the Poincaré distance formula $d_{\mathbb{B}}$ defined in Eq. (1). Then, we apply \textbf{Multidimensional Scaling (MDS)} to project these hyperbolic points into a 2D Euclidean space. 
Each LDN is colored based on its \textit{Cover Rate} (feature importance) provided by a pre-trained XGBoost model, representing its relative level in the tree hierarchy. As illustrated in Figure \ref{fig:hde_vis} for $ozone\_level$ and $allbp$:
\begin{itemize}
	\item \textbf{Fractal Emergence}: Starting from a random initialization (Epoch 0), the LDNs progressively organize into distinct clusters with fractal-like branching patterns, mirroring the topology of decision trees. 
	\item \textbf{Hierarchy-Aware Distribution}: Highly important LDNs (darker red, higher cover rate) tend to reside near the center of the space, effectively acting as ``root-like'' nodes with broader influence. Conversely, LDNs with lower importance (lighter yellow) are pushed towards the boundary, representing ``leaf-like'' fine-grained decision regions. This spatial arrangement serves as direct evidence that the hyperbolic manifold constraint forces the model to induce a latent reasoning hierarchy equivalent to that of GBDTs.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{EmbeddingVisual.png}
    \caption{Hyperbolic LDN distribution visualized via MDS and colored by XGBoost Cover Rate. The emergence of fractal structures and the center-to-boundary importance gradient confirm the induction of a tree-like hierarchy.}
    \label{fig:hde_vis}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we address the fundamental "Dual Deficiency" of deep tabular learning, representation granularity mismatch and the lack of intrinsic hierarchical geometry. By proposing \textbf{Hyperbolic Decision Embedding (HDE)}, we establish a novel \textit{geometric isomorphism} between neural embeddings and the robust reasoning logic of decision trees. Our framework unifies heterogeneous features into a standardized set of \textbf{Latent Decision Nodes (LDNs)} within the Poincaré ball, where \textbf{Soft Decision Routing} elevates numerical feature modeling from a coarse feature-level projection to a fine-grained value-level discretization. 

Through extensive experiments on the 30 datasets of the TALENT-tiny-core benchmark, our full model \textbf{HDE-Net} achieves \textbf{State-of-the-Art (Rank 1st)} performance, consistently outperforming both the industrial gold standard GBDTs and leading tabular DNN baselines. Beyond performance metrics, our qualitative visualizations provide empirical proof that HDE-Net induces a tree-equivalent hierarchy directly within its embedding space, where important decision boundaries align with those of traditional trees. This ``heavy-representation, light-backbone'' design demonstrates that deep models can surpass tree-based systems without resorting to bloated architectures or expensive retrieval mechanisms. We believe that HDE provides a promising geometric foundation for future tabular learning research, and we plan to explore its scalability to cross modality datasets and its potential for foundation model integration in future work.

\iffalse
\section{Introduction}
ACM's consolidated article template, introduced in 2017, provides a
consistent \LaTeX\ style for use across ACM publications, and
incorporates accessibility and metadata-extraction functionality
necessary for future Digital Library endeavors. Numerous ACM and
SIG-specific \LaTeX\ templates have been examined, and their unique
features incorporated into this single new template.

If you are new to publishing with ACM, this document is a valuable
guide to the process of preparing your work for publication. If you
have published with ACM before, this document provides insight and
instruction into more recent changes to the article template.

The ``\verb|acmart|'' document class can be used to prepare articles
for any ACM publication --- conference or journal, and for any stage
of publication, from review to final ``camera-ready'' copy, to the
author's own version, with {\itshape very} few changes to the source.

\section{Template Overview}
As noted in the introduction, the ``\verb|acmart|'' document class can
be used to prepare many different kinds of documentation --- a
double-anonymous initial submission of a full-length technical paper, a
two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
journal article, a SIGCHI Extended Abstract, and more --- all by
selecting the appropriate {\itshape template style} and {\itshape
  template parameters}.

This document will explain the major features of the document
class. For further information, the {\itshape \LaTeX\ User's Guide} is
available from
\url{https://www.acm.org/publications/proceedings-template}.

\subsection{Template Styles}

The primary parameter given to the ``\verb|acmart|'' document class is
the {\itshape template style} which corresponds to the kind of publication
or SIG publishing the work. This parameter is enclosed in square
brackets and is a part of the {\verb|documentclass|} command:
\begin{verbatim}
  \documentclass[STYLE]{acmart}
\end{verbatim}

Journals use one of three template styles. All but three ACM journals
use the {\verb|acmsmall|} template style:
\begin{itemize}
\item {\texttt{acmsmall}}: The default journal template style.
\item {\texttt{acmlarge}}: Used by JOCCH and TAP.
\item {\texttt{acmtog}}: Used by TOG.
\end{itemize}

The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
\begin{itemize}
\item {\texttt{sigconf}}: The default proceedings template style.
\item{\texttt{sigchi}}: Used for SIGCHI conference articles.
\item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
\end{itemize}

\subsection{Template Parameters}

In addition to specifying the {\itshape template style} to be used in
formatting your work, there are a number of {\itshape template parameters}
which modify some part of the applied template style. A complete list
of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

Frequently-used parameters, or combinations of parameters, include:
\begin{itemize}
\item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
  conference submission. Anonymizes the work and includes line
  numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
  submission's unique ID on each page of the work.
\item{\texttt{authorversion}}: Produces a version of the work suitable
  for posting by the author.
\item{\texttt{screen}}: Produces colored hyperlinks.
\end{itemize}

This document uses the following string as the first command in the
source file:
\begin{verbatim}
\documentclass[sigconf,authordraft]{acmart}
\end{verbatim}

\section{Modifications}

Modifying the template --- including but not limited to: adjusting
margins, typeface sizes, line spacing, paragraph and list definitions,
and the use of the \verb|\vspace| command to manually adjust the
vertical spacing between elements of your work --- is not allowed.

{\bfseries Your document will be returned to you for revision if
  modifications are discovered.}

\section{Typefaces}

The ``\verb|acmart|'' document class requires the use of the
``Libertine'' typeface family. Your \TeX\ installation should include
this set of packages. Please do not substitute other typefaces. The
``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
as they will override the built-in typeface families.

\section{Title Information}

The title of your work should use capital letters appropriately -
\url{https://capitalizemytitle.com/} has useful rules for
capitalization. Use the {\verb|title|} command to define the title of
your work. If your work has a subtitle, define it with the
{\verb|subtitle|} command.  Do not insert line breaks in your title.

If your title is lengthy, you must define a short version to be used
in the page headers, to prevent overlapping text. The \verb|title|
command has a ``short title'' parameter:
\begin{verbatim}
  \title[short title]{full title}
\end{verbatim}

\section{Authors and Affiliations}

Each author must be defined separately for accurate metadata
identification.  As an exception, multiple authors may share one
affiliation. Authors' names should not be abbreviated; use full first
names wherever possible. Include authors' e-mail addresses whenever
possible.

Grouping authors' names or e-mail addresses, or providing an ``e-mail
alias,'' as shown below, is not acceptable:
\begin{verbatim}
  \author{Brooke Aster, David Mehldau}
  \email{dave,judy,steve@university.edu}
  \email{firstname.lastname@phillips.org}
\end{verbatim}

The \verb|authornote| and \verb|authornotemark| commands allow a note
to apply to multiple authors --- for example, if the first two authors
of an article contributed equally to the work.

If your author list is lengthy, you must define a shortened version of
the list of authors to be used in the page headers, to prevent
overlapping text. The following command should be placed just after
the last \verb|\author{}| definition:
\begin{verbatim}
  \renewcommand{\shortauthors}{McCartney, et al.}
\end{verbatim}
Omitting this command will force the use of a concatenated list of all
of the authors' names, which may result in overlapping text in the
page headers.

The article template's documentation, available at
\url{https://www.acm.org/publications/proceedings-template}, has a
complete explanation of these commands and tips for their effective
use.

Note that authors' addresses are mandatory for journal articles.

\section{Rights Information}

Authors of any work published by ACM will need to complete a rights
form. Depending on the kind of work, and the rights management choice
made by the author, this may be copyright transfer, permission,
license, or an OA (open access) agreement.

Regardless of the rights management choice, the author will receive a
copy of the completed rights form once it has been submitted. This
form contains \LaTeX\ commands that must be copied into the source
document. When the document source is compiled, these commands and
their parameters add formatted text to several areas of the final
document:
\begin{itemize}
\item the ``ACM Reference Format'' text on the first page.
\item the ``rights management'' text on the first page.
\item the conference information in the page header(s).
\end{itemize}

Rights information is unique to the work; if you are preparing several
works for an event, make sure to use the correct set of commands with
each of the works.

The ACM Reference Format text is required for all articles over one
page in length, and is optional for one-page articles (abstracts).

\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).

\section{Sectioning Commands}

Your work should use standard \LaTeX\ sectioning commands:
\verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
\verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
\verb|\subsusection| should be numbered; do not remove the numbering
from the commands.

Simulating a sectioning command by setting the first word or words of
a paragraph in boldface or italicized text is {\bfseries not allowed.}

Below are examples of sectioning commands.

\subsection{Subsection}
\label{sec:subsection}

This is a subsection.

\subsubsection{Subsubsection}
\label{sec:subsubsection}

This is a subsubsection.

\paragraph{Paragraph}

This is a paragraph.

\subparagraph{Subparagraph}

This is a subparagraph.

\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
high-quality tables.

Table captions are placed {\itshape above} the table.

Because tables cannot be split across pages, the best placement for
them is typically the top of the page nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and the
table caption.  The contents of the table itself must go in the
\textbf{tabular} environment, to be aligned properly in rows and
columns, with the desired horizontal and vertical rules.  Again,
detailed instructions on \textbf{tabular} material are found in the
\textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed output of
this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}

Always use midrule to separate table header rows from data rows, and
use it only for this purpose. This enables assistive technologies to
recognise table headers and support their users in navigating tables
more easily.

\section{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of the three are
discussed in the next sections.

\subsection{Inline (In-text) Equations}
A formula that appears in the running text is called an inline or
in-text formula.  It is produced by the \textbf{math} environment,
which can be invoked with the usual
\texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
and structures, from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
examples of in-text equations in context. Notice how this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols and
structures available in \LaTeX\@; this section will just give a couple
of examples of display equations in context.  First, consider the
equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or
more images can be placed within a figure. If your figure contains
third-party material, you must clearly identify it as such, as shown
in the example below.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sample-franklin}
  \caption{1907 Franklin Model D roadster. Photograph by Harris \&
    Ewing, Inc. [Public domain], via Wikimedia
    Commons. (\url{https://goo.gl/VLCRBB}).}
  \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Your figures should contain a caption which describes the figure to
the reader.

Figure captions are placed {\itshape below} the figure.

Every figure should also have a figure description unless it is purely
decorative. These descriptions convey what’s in the image to someone
who cannot see it. They are also used by search engine crawlers for
indexing images, and when images cannot be loaded.

A figure description must be unformatted plain text less than 2000
characters long (including spaces).  {\bfseries Figure descriptions
  should not repeat the figure caption – their purpose is to capture
  important information that is not already provided in the caption or
  the main text of the paper.} For figures that convey important and
complex new information, a short text description may not be
adequate. More complex alternative descriptions can be placed in an
appendix and referenced in a short figure description. For example,
provide a data table capturing the information in a bar chart, or a
structured list representing a graph.  For additional information
regarding how best to write figure descriptions and why doing this is
so important, please see
\url{https://www.acm.org/publications/taps/describing-figures/}.

\subsection{The ``Teaser Figure''}

A ``teaser figure'' is an image, or set of images in one figure, that
are placed after all author and affiliation information, and before
the body of the article, spanning the page. If you wish to have such a
figure in your article, place the command immediately before the
\verb|\maketitle| command:
\begin{verbatim}
  \begin{teaserfigure}
    \includegraphics[width=\textwidth]{sampleteaser}
    \caption{figure caption}
    \Description{figure description}
  \end{teaserfigure}
\end{verbatim}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}


  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  presentation~\cite{Reiser2014}. An article under
  review~\cite{Baggett2025}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
  Artifacts: \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

Identification of funding sources and other support, and thanks to
individuals and groups that assisted in the research and the
preparation of the work should be included in an acknowledgment
section, which is placed just before the reference section in your
document.

This section has a special environment:
\begin{verbatim}
  \begin{acks}
  ...
  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{description}
\item[\texttt{sidebar}:]  Place formatted text in the margin.
\item[\texttt{marginfigure}:] Place a figure in the margin.
\item[\texttt{margintable}:] Place a table in the margin.
\end{description}
\fi

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{unsrt}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section{Statistics of the Datasets}
\label{app:datasets}
Table \ref{tab:datasets} presents detailed statistics of the datasets in the TALENT-tiny-core classification benchmark. Class denotes the number of target classes, Cat the number of categorical features, Num the number of numerical features, and Size the dataset size. All statistics are obtained from the official TALENT GitHub repository.
\begin{table}[htbp]
\small  \centering  \caption{Detailed statistics of the datasets in the TALENT-tiny-core classification benchmark}
  	\scalebox{1.0}{    \begin{tabular}{rrrrr}    \toprule    \multicolumn{1}{l}{\textbf{name}} & \multicolumn{1}{l}{\textbf{Class}} & \multicolumn{1}{l}{\textbf{Cat}} & \multicolumn{1}{l}{\textbf{Num}} & \multicolumn{1}{l}{\textbf{Size}} \\    \midrule    \multicolumn{1}{l}{ada} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{48} & \multicolumn{1}{c}{3317} \\    \multicolumn{1}{l}{airlines} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{1600} \\    \multicolumn{1}{l}{allbp} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{3017} \\    \multicolumn{1}{l}{ASP-POTASSCO} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{140} & \multicolumn{1}{c}{1035} \\    \multicolumn{1}{l}{autoUniv-au7-1100} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{880} \\    \multicolumn{1}{l}{company\_bankruptcy} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{93} & \multicolumn{1}{c}{5455} \\    \multicolumn{1}{l}{eucalyptus} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{14} & \multicolumn{1}{c}{588} \\    \multicolumn{1}{l}{Gender\_Gap\_in\_Spanish} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{3796} \\    \multicolumn{1}{l}{hill-valley} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{969} \\    \multicolumn{1}{l}{house\_16H} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{16} & \multicolumn{1}{c}{10790} \\    \multicolumn{1}{l}{ibm-employee-performance} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{1176} \\    \multicolumn{1}{l}{INNHotelsGroup} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{29020} \\    \multicolumn{1}{l}{internet\_firewall} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{52425} \\    \multicolumn{1}{l}{jasmine} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{136} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{2387} \\    \multicolumn{1}{l}{jungle\_chess\_2pcs} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{35855} \\    \multicolumn{1}{l}{law-school-admission} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{16640} \\    \multicolumn{1}{l}{microaggregation2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{20} & \multicolumn{1}{c}{16000} \\    \multicolumn{1}{l}{national-longitudinal} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{3926} \\    \multicolumn{1}{l}{okcupid\_stem} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{21341} \\    \multicolumn{1}{l}{online\_shoppers} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{9864} \\    \multicolumn{1}{l}{ozone\_level} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{36} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{2028} \\    \multicolumn{1}{l}{pc4} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{37} & \multicolumn{1}{c}{1166} \\    \multicolumn{1}{l}{PhishingWebsites} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{30} & \multicolumn{1}{c}{8844} \\    \multicolumn{1}{l}{rice\_cammeo\_\&\_osmancik} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{3048} \\    \multicolumn{1}{l}{shill-bidding} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5056} \\    \multicolumn{1}{l}{Shipping} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{8799} \\    \multicolumn{1}{l}{statlog} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{800} \\    \multicolumn{1}{l}{thyroid} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{5760} \\    \multicolumn{1}{l}{waveform} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{4000} \\    \multicolumn{1}{l}{wine-quality-red} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{1279} \\    \midrule          &       &       &       &  \\    \end{tabular}%
    }  \label{tab:datasets}%\end{table}%

\section{Models for Comparison}
\label{app:comp_models}
Table \ref{tab:comp_models} lists all models included in the ranking, with results provided by the official TALENT project. Note that some recent models, such as TabM and TabPFN v2, are excluded because their official benchmark results on TALENT have not yet been released, although their code has been integrated into the framework. We report full dataset-level results for HDE-Net to facilitate future comparisons.
\begin{table}[]
\caption{Models for Comparison}
\label{tab:comp_models}
\begin{tabular}{|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Traditional \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}dummy, LogReg, NCM, NaiveBayes,\\ knn, svm, xgboost, catboost,\\ RandomForest, lightgbm\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Deep Learning \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}tabpfn, mlp, resnet, node, switchtab,\\ tabnet, tabcaps, tangos, danets, ftt,\\ autoint, dcn2, snn, tabtransformer,\\ ptarl, grownet, tabr, modernNCA,\\ mlp\_plr, realmlp, excelformer\end{tabular} \\ \hline
\end{tabular}
\end{table}

\section{Research Methods}

\subsection{Part One}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
lacinia dolor. Integer ultricies commodo sem nec semper.

\subsection{Part Two}

Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
eros. Vivamus non purus placerat, scelerisque diam eu, cursus
ante. Etiam aliquam tortor auctor efficitur mattis.

\section{Online Resources}

Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
enim maximus. Vestibulum gravida massa ut felis suscipit
congue. Quisque mattis elit a risus ultrices commodo venenatis eget
dui. Etiam sagittis eleifend elementum.

Nam interdum magna at lectus dignissim, ac dignissim lorem
rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
