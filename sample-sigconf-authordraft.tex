%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%\documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf,anonymous,review,nonacm]{acmart}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{threeparttable}
\usepackage{bm}

\usepackage{url}
\usepackage{hyperref} % 用于制作可点击的超链接

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Manifold Constrained Tabular Deep Neural Networks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract} 


Despite the success of Tabular Deep Neural Network (DNN) models, Gradient-Boosted Decision Tree (GBDT) models remain the dominant baseline for tabular data, largely due to their efficiency in processing heterogeneous features and hierarchical reasoning. The critical bottleneck in tabular DNNs is feature representation granularity mismatch and inability to compensate for hierarchical reasoning. 
To bridge this gap, we propose a novel \textit{Hyperbolic Decision Embedding Network (HDE-Net)} that simulates a geometric isomorphism between feature embeddings and decision-tree inference. HDE itself unifies feature embedding by treating both categorical values and numerical ranges as \textit{Latent Decision Nodes (LDNs)} within the Poincaré ball hyperbolic representation. Specifically, we introduce \textit{Soft Decision Routing} to semantically discretize numerical features, aligning them with the value-level granularity of categorical embeddings. By leveraging the exponential growth in hyperbolic geometry, HDE-Net implicitly enforces hierarchical constraints, mimicking tree-like decision boundaries in the continuous-valued space. Furthermore, we incorporate an \textit{entropy-aware} mechanism to adaptively allocate semantic capacity for numerical LDNs. Our model achieves state-of-the-art performance on the TALENT-tiny-core classification benchmark, comprising 30 datasets. HDE-Net emerged \textit{ranks 1st} among both tree-based and tabular DNN baselines.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010342.10010343</concept_id>
       <concept_desc>Computing methodologies~Modeling methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Modeling methodologies}

%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>00000000.0000000.0000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Tabular Representation Learning, Geometric Deep Learning, Deep Tabular Learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

Tabular data remains the cornerstone of critical real-world applications, from financial risk assessment to healthcare diagnostics \cite{guo2017deepfm, buczak2015survey, hyland2020early, liu2025talent}. Unlike images or text, tabular data is characterized by heterogeneous features, dense semantics, and lacks of spatial or sequential invariance. While Deep Neural Networks (DNNs) have revolutionized computer vision and NLP, their dominance has not fully transferred to the tabular domain \cite{yan2023t2g, ye2024revisiting}. Recent extensive benchmarks \cite{liu2025talent} and studies \cite{ye2024revisiting, ye2025closer} consistently demonstrate that Gradient Boosted Decision Trees (GBDTs), such as XGBoost \cite{chen2016xgboost} and CatBoost \cite{prokhorenkova2018catboost}, remain the dominant baseline in both accuracy and efficiency.

Why do GBDTs still maintain this edge? We argue that their success hinges on two fundamental principles: \textit{1). Unified Node Representation}: Despite the heterogeneity of raw features, tree models abstract both categorical values and continuous numerical ranges into a unified formalism: discrete tree nodes. This unification prevents the model from being biased towards specific feature types, ensuring robust performance across diverse tabular scenarios. \textit{2). Hierarchical Reasoning}: The tree-like reasoning mechanism naturally captures non-linear decision boundaries and high-order interactions through hierarchical paths.

In contrast, existing tabular DNNs suffer from two issues: \textit{1.) Representation Granularity Mismatch}: While categorical features are naturally modeled at the fine-grained ``value-level'' (via lookup tables for each unique value), numerical features are typically modeled at the coarse ``feature-level'' (e.g., a single linear projection shared across all values of a feature, or even simple raw-value concatenation \cite{huang2020tabtransformer, arik2021tabnet, gorishniy2022embeddings}). This restricts numerical representation to a linear value space, failing to capture the non-linear semantics inherent in data, and thereby forcing subsequent layers to compensate for this semantic deficiency. \textit{2.) geometric incompatibility}: DNNs operate in Euclidean space, whose geometry is ill-suited to model the exponential branching structure of decision trees, leading to a fundamental geometric mismatch between representation space and data structure.

Prior research attempts to address these issues through ad-hoc feature fusion \cite{holzmuller2025realmlp, wang2021dcn} or pseudo-alignment via tokenizers \cite{gorishniy2021revisiting, yan2023t2g}. However, these paradigms often rely on either heuristic adjustments or bloated backbones to compensate for representation limitations, which often makes the models sensitive to specific feature types and difficult to generalize across datasets.

To resolve these two issues of granularity and geometry, we propose \textbf{Hyperbolic Decision Embedding Network (HDE-Net)}. Our framework establishes a geometric isomorphism with the inference logic of decision trees, achieving true value-level unification for heterogeneous features using Poincaré ball embeddings \cite{nickel2017poincare, becigneul2018riemannian, peng2021hyperbolic}.

Our approach rests on two key innovations. First, we introduce \textit{Soft Decision Routing} for numerical features, which dynamically maps continuous values to \textit{Latent Decision Nodes (LDNs)}, simulating value range splitting and unifying the representation granularity of both feature types. Second, by embedding these nodes in hyperbolic space to construct \textit{Hyperbolic Decision Embeddings (HDE)}, we leverage its exponential volume growth to implicitly enforce hierarchical constraints. 
Additionally, we design an \textit{Entropy-aware Capacity Allocation} algorithm to dynamically allocate semantic capacity (number of LDNs) for each numerical feature. Finally, we construct HDE-Net by coupling the expressive HDE with a simple MLP predictor. 

Our main contributions are:
\begin{itemize}
    \item \textbf{Differentiable Soft Discretization}: We propose representing numerical features as weighted aggregations of LDNs. This paradigm shifts numerical processing from coarse linear projections to fine-grained, value-level embeddings, effectively aligning them with categorical representations.
    \item \textbf{Implicit Hierarchical Constraint}: We leverage the intrinsic curvature of hyperbolic space to enforce hierarchical constraints without explicit graphs guidance.
    \item \textbf{Adaptive Semantic Capacity Allocation}: We introduce the entropy-aware capacity allocation algorithm to balance numerical LDNs' expressiveness and efficiency, preventing over- and under-parameterization.
    \item \textbf{SOTA Performance \& Efficiency}: HDE-Net achieves SOTA results on the TALENT-tiny-core classification benchmark, surpassing CatBoost, RealMLP, and ModernNCA. 
\end{itemize}

The rest of this paper is organized as follows. Section \ref{sec:related_work} reviews related work of current tabular DNNs and Hyperbolic DNNs. Section \ref{sec:preliminaries} provides preliminaries on hyperbolic geometry and necessary operators. Section \ref{sec:method} details the proposed HDE-Net. Section \ref{sec:exp} presents experimental results and analysis, followed by conclusions in Section \ref{sec:conclusion}.

\section{Related Work}
\label{sec:related_work}

\subsection{Paradigms in Deep Tabular Learning}
Recent advances in Deep Tabular Learning can be categorized into four paradigms. 
\textit{(1) Ad-hoc Fusion} applies distinct processing pipelines to different feature types. For instance, RealMLP \cite{holzmuller2025realmlp} employs Periodic-Bias-Linear-Dense (PBLD) embeddings for numerical features and value-level embeddings for categorical ones, fused via residual blocks. TabTransformer \cite{huang2020tabtransformer} processes categorical features via Transformers while concatenating raw numerical features directly. 
\textit{(2) Pseudo-Alignment} forces dimensional alignment via tokenizers. The representative FT-Transformer \cite{gorishniy2021revisiting} projects numerical features via linear layers ($x \cdot W + b$) and looks up categorical embeddings, processing them uniformly through a deep Transformer backbone. Similar mechanisms are adopted by AutoInt \cite{song2019autoint}, ExcelFormer \cite{chen2023excelformer}, and T2G-Former \cite{yan2023t2g}.
\textit{(3) Retrieval-based Methods}, such as ModernNCA \cite{ye2024revisiting} and TabR \cite{gorishniy2023tabr}, enhance prediction by retrieving similar samples from the training set. While achieving high performance, they incur high inference latency and storage costs, and are sensitive to out-of-distribution shifts.
\textit{(4) Foundation Models} like TabPFN \cite{hollmann2022tabpfn} leverage Prior-Data Fitted Networks (PFNs) pre-trained on synthetic datasets. Despite their zero-shot capabilities, they face scalability bottlenecks regarding context length ($N$) and feature dimensions.

\subsection{Empirical Analysis of Architectural Limitations}
To rigorously analyze the intrinsic architectural efficacy, we focus on the first two paradigms (Ad-hoc Fusion and Pseudo-Alignment) and compare them against GBDTs under comparable computational budgets. We exclude Retrieval and Foundation models from this specific analysis due to their discrepancy on research focus and resource requirements. We conduct a performance breakdown on the TALENT benchmark (see Table \ref{tab:motivation}) across datasets grouped by feature composition: \textit{Overall} (All datasets), \textit{Num-Only} (Datasets contains only numerical features), \textit{Num-Heavy} (Numerical dominant datasets), and \textit{Cat-Heavy} (Categorical dominant datasets). The results reveal critical insights: 
\begin{itemize}
	\item \textbf{GBDT Dominance:} Tree models remain the superior baseline, especially in \textit{Cat-Heavy} scenarios where they dominate (e.g., XGBoost Rank 3.0). Their performance degrades only in \textit{Num-Only} tasks.
	\item \textbf{The Specialization Trap of Ad-hoc Fusion:} RealMLP achieves SOTA performance on \textit{Num-Only} datasets (Rank 6.4), validating its numerical embedding. Even the simplest MLP can defeat GBDTs' performance. However, all their performances collapse on \textit{Cat-Heavy} domains (RealMLP Rank 12.9, MLP Rank 21.0). This volatility indicates a failure to unify heterogeneous information within a coherent reasoning framework.
	\item \textbf{The Inefficiency of Pseudo-Alignment:} FT-Transformer demonstrates stability across feature types, validating the utility of alignment. However, its inferior performance on numerical tasks (Rank 12.3) confirms that simple linear projections are a bottleneck. Furthermore, despite utilizing a significantly heavier backbone, it fails to surpass simpler MLP-based architectures like RealMLP in overall ranking.
\end{itemize}

\begin{table}[tbp]
\small
  \centering
  \caption{Performance breakdown on the TALENT-tiny core classification benchmark by feature composition. Values represent Average Rank (lower is better). Parentheses indicate rank shifts relative to the \textbf{Overall} performance (\textcolor{green}{Green/-}: Improved Rank; \textcolor{red}{Red/+}: Worsened Rank).}
  	\scalebox{0.8}{
    \begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Models}} & \textbf{Overall} & \textbf{Num-Only} & \textbf{Num-Heavy} & \textbf{Cat-Heavy} \\
    \midrule
    \multicolumn{1}{c}{\multirow{3}[2]{*}{\makecell*[c]{Tree\\Models}}} 
          & XGBoost & 7.4  & 10.5 \textcolor{red}{(+3.1)} & 5.9 \textcolor{green}{(-1.5)} & 3.0 \textcolor{green}{(-4.3)} \\
          & LightGBM & 8.4  & 11.0 \textcolor{red}{(+2.6)} & 6.5 \textcolor{green}{(-1.9)} & 5.5 \textcolor{green}{(-2.8)} \\
          & CatBoost & 8.6  & 12.3 \textcolor{red}{(+3.7)} & 6.4 \textcolor{green}{(-2.2)} & 3.9 \textcolor{green}{(-4.7)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Ad-hoc\\Fusion}}} 
          & RealMLP & 8.9  & 6.4 \textcolor{green}{(-2.5)} & 9.7 \textcolor{red}{(+0.8)} & 12.9 \textcolor{red}{(+4.0)} \\
          & MLP   & 14.9 & 10.1 \textcolor{green}{(-4.8)} & 17.8 \textcolor{red}{(+2.8)} & 21.0 \textcolor{red}{(+6.1)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Pseudo\\Alignment}}} 
          & FT-Transformer & 11.6 & 12.3 \textcolor{red}{(+0.6)} & 11.3 \textcolor{green}{(-0.3)} & 10.7 \textcolor{green}{(-0.9)} \\
          & ExcelFormer & 13.9 & 14.7 \textcolor{red}{(+0.8)} & 13.3 \textcolor{green}{(-0.6)} & 13.2 \textcolor{green}{(-0.7)} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:motivation}%
\end{table}%


\subsection{Hyperbolic Geometry in Representation Learning}
\label{sec:rel_hyperbolic}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{Arch.png} 
  \vskip -10pt
  \caption {\textbf{Geometric Isomorphism between Decision Trees and Hyperbolic Decision Embedding (HDE).} 
\textbf{(Left)} Example DT for mood disorder prediction. 
\textbf{(Middle)} Latent Decision Nodes (LDNs) embedded in the Poincaré ball: categorical values (e.g., \textcolor{red}{Male}, \textcolor{teal}{Female}) map directly to LDNs, while numerical features (e.g., Age) are softly routed to multiple LDNs to represent multiple ranges split just like DT.
\textbf{(Right)} HDE-Net inference: hyperbolic embeddings (\textit{solid}) are mapped to the Euclidean tangent space (\textit{dashed}) via $\log_{\mathbf{0}}(\cdot)$ for efficient weighted aggregation (just for numerical features) and MLP classification.}
  \label{fig:arch}
  \vskip -10pt
\end{figure*}

Hyperbolic space is mathematically isomorphic to a continuous tree due to its exponential volume growth \cite{nickel2017poincare}. This property has driven a lot of representation learning in domains with explicit hierarchical structures, such as Graph Learning \cite{chami2019hyperbolic, dai2021hyperbolic}, Computer Vision \cite{lensink2022fully}, and Multimodal Learning (e.g., MERU \cite{desai2023hyperbolic}). General-purpose neural components, such as Hyperbolic Linear Layer \cite{shimizu2020hyperbolic}, have also been proposed to facilitate these applications.

Closer to our work, recent studies have extended traditional machine learning to non-Euclidean geometries. Notably, HyperDT \cite{chlenski2023fast} generalizes decision trees to hyperbolic space, and PXgboost \cite{suganthan2025euclidean} extends XGBoost from Euclidean space to the Poincaré hyperbolic space by replacing Euclidean gradients and Hessians with Riemannian counterparts. These methods validates the geometric alignment between tree-based splitting and hyperbolic manifolds. However, they operate as traditional classifiers; they lack the \textit{end-to-end representation learning} capabilities of DNNs and cannot seamlessly integrate other unstructured modalities.

The application of hyperbolic geometry to Tabular DNNs, particularly for feature representation, remains a significant unexplored. HDE resolves the granularity and geometry mismatch issues. By mapping features to LDNs in the Poincaré ball, we empower a simple MLP backbone with the hierarchical reasoning logic of decision trees, combining geometric interpretability with deep representation power.

\section{Preliminaries}
\label{sec:preliminaries}

\textbf{Problem Formulation.} 
We consider a supervised tabular learning task with a dataset $\mathcal{D} = \{(\bm{x}_i, \bm{y}_i)\}_{i=1}^N$. Each input instance $\bm{x}$ consists of heterogeneous parts: numerical features $\bm{x}^{\text{num}} \in \mathbb{R}$ and categorical features $\bm{x}^{\text{cat}} \in \mathbb{C}$, where $\mathbb{C}$ denotes the category domain. Our goal is to learn a function $f(\bm{x})$ that predicts the target $\bm{y}$. As discussed, we aim to unify the representation of these heterogeneous features into a shared geometric space.

\noindent\textbf{The Poincaré Ball Model.} 
Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature. We utilize the $d$ dimensional Poincaré ball model $(\mathbb{B}_c^d, g^{\mathbb{B}})$, defined as the manifold $\mathbb{B}_c^d = \{ \bm{x} \in \mathbb{R}^d : c\| \bm{x} \|^2 < 1 \}$, where $c$ is a hyperparameter controlling the curvature (learnable). The induced distance between two points $\bm{u}, \bm{v} \in \mathbb{B}_c^d$ is given by:
\begin{equation}
    d_{\mathbb{B}}(\bm{u}, \bm{v}) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c} \| -\bm{u} \oplus_c \bm{v} \|),
\end{equation}
where $\oplus_c$ denotes the Möbius addition \cite{ungar2008analytic}. A key property of this space is that its volume grows exponentially with the radius. This aligns perfectly with the exponential growth of nodes in a decision tree as depth increases, making it an ideal continuous proxy for hierarchical tree structures \cite{nickel2017poincare}.

\noindent\textbf{Hyperbolic Embedding: Poincaré vs. Lorentz.} 
While the Lorentz model is often preferred for numerical stability in complex GNNs \cite{yang2022hyperbolic}, HDE avoids complex manifold operations (e.g., inter-node addition) and frequent Euclidean-Hyperbolic projections. We therefore select the Poincaré ball for its conformal property to Euclidean space (angle-preserving) \cite{ganea2018hyperbolic}. This allows us to intuitively encode hierarchy via the norm (depth) and logical affinity via angles (sibling relations), achieving a direct geometric isomorphism with decision trees.

\noindent\textbf{Tangent Space Projection.} 
To integrate hyperbolic embeddings with Euclidean neural networks (i.e., the MLP backbone), we utilize the \textit{Logarithmic Map} $\log_{\mathbf{0}}(\cdot)$ to project points from the manifold to the Euclidean tangent space at the origin $\mathcal{T}_{\mathbf{0}}\mathbb{B}_c^d$:
\begin{equation}
    \log_{\mathbf{0}}(\bm{u}) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c}\|\bm{u}\|) \frac{\bm{u}}{\|\bm{u}\|}.
\end{equation}
In HDE, we directly initialize LDNs on the manifold and use $\log_{\mathbf{0}}(\cdot)$ as the bridge to the Euclidean predictor.

\section{Methodology}
\label{sec:method}

The HDE-Net (Fig~\ref{fig:arch}) consists of three key stages: (1) \textit{Semantic Discretization}, where heterogeneous features are routed to a unified set of Latent Decision Nodes (LDNs); (2) \textit{Hyperbolic Embedding}, where LDNs are optimized on the Poincaré ball to enforce implicit hierarchical constraints; and (3) \textit{Tangent Projection \& Prediction}, where embeddings are projected to the Euclidean tangent space for efficient aggregation and classification.  Within the Semantic Discretization stage, we introduce two novel mechanisms specifically for numerical features: \textit{Entropy-Aware Capacity Allocation} to determine the number of LDNs, and \textit{Soft Decision Routing} to mimic tree-based interval discretization.

\subsection{Entropy-Aware Capacity Allocation}
\label{sec:entropy}
Assigning a fixed number of LDNs ($k$) to all numerical features leads to suboptimal resource allocation, simple features may suffer from over-parameterization, while complex ones face under-parameterization. To address this, we propose the \textit{Entropy-aware Capacity Allocation} mechanism. We dynamically determine $k_j$ for each numerical feature $\bm{x}_j^{num}$ based on its information entropy $H(x_j^{num})$ and effective sample size $N_{eff}$ (without missing values). Finally, we obtain the capacity map $\bm{K}$ for all numerical features. This ensures that the model allocates more decision bandwidth to features with richer information, akin to how decision trees grow deeper for critical features. The detailed procedure is outlined in Algorithm \ref{alg:entropy}.

\subsection{Soft Decision Routing}
\label{sec:sdr}
For numerical features, we simulate the splitting process of decision trees to achieve semantic discretization. Given an numerical feature input $\bm{x}^{(num)}_j$, we employ a learnable gating mechanism, termed \textit{Soft Decision Routing}, to compute $k_j$ weights to couple with $k_j$ LDNs. Specifically, we define the routing weights $\bm{W}_j \in \mathbb{R}^{k_j}$ as:
\begin{equation}
    \bm{W}_j = \boldsymbol{a}_j x^{num}_j + \boldsymbol{b}_j, \quad \boldsymbol{W}_j \in \mathbb{R}^{k_j}
    \label{eq:router_weight}
\end{equation}
where $\boldsymbol{a}, \boldsymbol{b}$ are learnable parameters. Taking Figure~\ref{fig:arch} as an example, $k$ and $\bm{W}$ are calculated for the "Age" column, and $j$ denotes the index of "Age" feature in numerical feature list. 
We then use $\bm{W}_j$ for weighted aggregation without a softmax or sigmoid activation. This design allows for flexible intensity scaling, preserving the magnitude information of the input. It effectively performs a soft aggregation operation rather than a hard selection. 

For categorical features $\bm{x}^{cat}$, the routing is deterministic, selecting the unique LDN corresponding to the category value. Consequently, categorical and numerical features are aligned in representational granularity, while numerical feature representations retain both linear (via intensity) and non-linear (via multiple LDNs) expressive capacity.

\begin{algorithm}[t]
\caption{Entropy-aware Capacity Allocation}
\label{alg:entropy}
\begin{algorithmic}[1]
\REQUIRE Dataset $\mathcal{D}$, Feature set $\bm{J}^{num}$, Feature input $\bm{x}^{num}$, Max capacity $k_{max}$, Min capacity $k_{min}$, 
\ENSURE Capacity map $\bm{K} = \{k_1, k_2, ..., k_j\}$
\FOR{each feature $j \in \bm{J}^{num}$}
    \STATE Calculate Shannon entropy $H(\bm{x}^{num}_j)$
    \STATE Count non-missing values $N_{eff}$
    \STATE Derive candidate capacity from entropy: $k_{ent} \leftarrow \max(k_{min}, \lfloor 2^{H(\bm{x}^{num}_j)} \rfloor)$
    \STATE Derive candidate capacity from samples: $k_{spl} \leftarrow \max(k_{min}, \lfloor \log_2(N_{eff}) \rfloor)$
    \STATE Determine final capacity: $k_j \leftarrow \min(k_{max}, k_{ent}, k_{spl})$
    \STATE Store $k_j$ in $\bm{K}$
\ENDFOR
\RETURN $\bm{K}$
\end{algorithmic}
\end{algorithm}
\vskip -10pt

\subsection{LDNs in Hyperbolic Space}
% We define the destinations of these routing paths as \textbf{Latent Decision Nodes (LDNs)}. 
Let $\mathcal{M} = \mathbb{B}_c^d$ be the $d$-dimensional Poincaré ball with curvature $c$. For all features' LDNs (numerical and categorical), we initialize their corresponding embeddings (HDE) $\bm{Z}$ directly on this manifold $\mathcal{M}$.
%\begin{equation}
%    \mathbf{Z}_j = \{\mathbf{z}_{j,1}, \dots, \mathbf{z}_{j,k_j}\} \in \mathbb{B}_c^d
%\end{equation}
By optimizing these nodes in hyperbolic space, HDE implicitly captures a hierarchical structure: general concepts (root-like) naturally cluster near the origin, while fine-grained values (leaf-like) are pushed towards the boundary. This geometric constraint enables the model to learn the hierarchical dependencies inherent in tabular data without explicit tree construction.

\subsection{Tangent Projection and Feature Aggregation}
To enable efficient inference and compatibility with standard neural layers, we decouple the geometric representations from subsequent computations by projecting them onto the tangent space. For categorical features, whose category values are deterministic, the retrieved HDE projections $e^{cat} = \log_{\mathbf{0}}(\bm{Z}^{cat})$ can be used directly.

For numerical features, we need to aggregates the corresponding HDEs using $\bm{W}$ calculated in Section~\ref{sec:sdr}. To avoid expensive Möbius operations on the manifold, we project all related LDNs to the Tangent space first, then perform weighted aggregation. Take numerical feature $j$ as an example, $\bm{Z}_j = \{z_0, ..., z_{k_j}\}, \quad \bm{Z}_j \subset \bm{Z}$. Its final Euclidean representation is:
\begin{equation}
	e_j^{num} = \frac{1}{k_j}\sum_{n=0}^{k_j}w_{j,n} \log_{\mathbf{0}}(z_{j, n}), \quad \{w_{j,0}, w_{j,1}, ..., w_{j,k_j}\} = \bm{W}_j
\end{equation}

\subsection{HDE-Net and its Hybrid Optimization Strategy}
To build the HDE-Net, the embeddings from all features $(e^{cat}, e^{num})$ are concatenated and fed into a standard MLP predictor. To train HDE-Net, we employ a \textit{Hybrid Optimization Strategy}. The LDN parameters $\mathbf{Z}$ residing on the manifold and the curvature $c$ are updated using Riemannian Adam \cite{becigneul2018riemannian}, which performs gradient updates in the manifold space $\mathcal{M}$. All other parameters (Routing weights $\boldsymbol{a}, \boldsymbol{b}$ and MLP weights) are updated using standard AdamW in Euclidean space. This hybrid approach ensures that the learned representations strictly adhere to hyperbolic geometry while maintaining the computational efficiency of Euclidean inference.

\section{Experiments}
\label{sec:exp}

\subsection{Setups}
\label{sec:setups}

% Table generated by Excel2LaTeX from sheet 'Sheet3'\begin{table*}[htbp]
\small  \centering  \caption{Performance of HDE-Net across all datasets in the TALENT-tiny core classification benchmark}
  \vskip -10pt
  \scalebox{0.86}{    \begin{tabular}{lccccclccccc}    \toprule    \multicolumn{12}{c}{\textbf{HDE-Net}} \\    \midrule    \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} & \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} \\    \midrule    ada   & 80.90\% & 80.12\% & 80.47\% & 89.44\% & 85.60\% & law-school-admission & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% \\    airlines & 63.92\% & 63.26\% & 63.21\% & 65.05\% & 64.30\% & microaggregation2 & 57.55\% & 39.19\% & 41.22\% & 82.00\% & 63.83\% \\    allbp & 83.81\% & 64.80\% & 71.42\% & 94.95\% & 97.58\% & national-longitudinal & 99.81\% & 99.80\% & 99.81\% & 99.99\% & 99.82\% \\    ASP-POTASSCO & 37.71\% & 35.77\% & 36.03\% & 80.66\% & 42.59\% & okcupid\_stem & 67.98\% & 51.01\% & 52.81\% & 80.55\% & 75.35\% \\    autoUniv-au7-1100 & 42.35\% & 40.82\% & 40.16\% & 69.24\% & 41.95\% & online\_shoppers & 83.46\% & 77.78\% & 80.18\% & 91.79\% & 90.49\% \\    company\_bankruptcy & 85.15\% & 69.17\% & 74.33\% & 95.99\% & 97.52\% & ozone\_level & 49.11\% & 49.90\% & 49.50\% & 68.64\% & 98.03\% \\    eucalyptus & 73.34\% & 70.64\% & 71.42\% & 93.50\% & 73.92\% & pc4   & 82.58\% & 69.05\% & 73.08\% & 93.08\% & 90.72\% \\    Gender\_Gap & 53.54\% & 42.88\% & 43.90\% & 65.37\% & 60.17\% & PhishingWebsites & 97.91\% & 97.74\% & 97.82\% & 99.71\% & 97.85\% \\    hill-valley & 73.33\% & 72.81\% & 72.65\% & 77.48\% & 72.80\% & rice\_cammeo\_\&\_osmancik & 93.11\% & 92.33\% & 92.65\% & 97.62\% & 92.86\% \\    house\_16H & 88.40\% & 88.39\% & 88.39\% & 94.69\% & 88.39\% & shill-bidding & 64.99\% & 56.06\% & 56.99\% & 71.46\% & 90.24\% \\    ibm-employee-performance & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% & Shipping & 75.97\% & 73.07\% & 68.75\% & 73.77\% & 69.05\% \\    INNHotelsGroup & 87.26\% & 86.21\% & 86.69\% & 94.49\% & 88.43\% & statlog & 68.87\% & 67.75\% & 68.20\% & 73.80\% & 74.05\% \\    internet\_firewall & 93.22\% & 77.66\% & 81.90\% & 98.85\% & 93.09\% & thyroid & 96.34\% & 97.60\% & 96.92\% & 99.88\% & 99.40\% \\    jasmine & 82.10\% & 79.37\% & 78.90\% & 84.06\% & 79.35\% & waveform\_version\_1 & 87.10\% & 87.09\% & 87.06\% & 96.85\% & 87.12\% \\    jungle\_chess\_2pcs & 97.67\% & 97.68\% & 97.66\% & 99.88\% & 98.20\% & wine-quality-red & 39.23\% & 33.01\% & 33.67\% & 75.98\% & 62.97\% \\    \bottomrule    \end{tabular}%
    }  \label{tab:HDE-Net_perf}%\end{table*}%

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{overall_performance.png}
    \vskip -10pt
    \caption{Critical Difference (CD) diagram on the TALENT-tiny-core classification benchmark. Average ranks are calculated via the Wilcoxon-Holm test ($\alpha=0.05$). HDE-Net ranks 1st (4.1667), demonstrating a significant performance lead over 31 baselines.}
    \label{fig:cd_diagram}
\end{figure*}

\textbf{Datasets.} 
We evaluate HDE-Net on the TALENT-tiny core classification benchmark \cite{liu2025talent}, which comprises 30 datasets (see Appendix \ref{app:datasets} for details). Unlike traditional benchmarks that are often biased toward numerical-dominant data, TALENT-tiny core provides a balanced and fair representation of various feature compositions (Numerical-only, Numerical-dominant, and Categorical-dominant). This ensures that the evaluation is not skewed by any specific feature type. Detailed statistical information for each dataset is provided in Appendix \ref{app:datasets}.

\noindent\textbf{Baselines.}
We include a comprehensive pool of 31 baselines whose results are officially disclosed on the TALENT leaderboard. These include 10 Traditional Models (e.g., CatBoost \cite{prokhorenkova2018catboost}, XGBoost \cite{chen2016xgboost}, LightGBM \cite{ke2017lightgbm}, Random Forest \cite{breiman2001random}, SVM \cite{hearst1998support}, KNN \cite{goldberger2004neighbourhood}) and 21 Deep Learning Models (e.g., ModernNCA \cite{ye2024revisiting}, RealMLP \cite{holzmuller2024better}, FT-Transformer \cite{gorishniy2021revisiting}, TabNet \cite{arik2021tabnet}). A full list of these baselines is provided in Appendix \ref{app:comp_models}.

\noindent\textbf{Hyperparameter Settings.}
For HDE-Net, the dimension of Hyperbolic Decision Embeddings is fixed at 12 across all datasets. The number of LDNs for each numerical feature is adaptively assigned via the Entropy-Aware Capacity Allocation mechanism (Sec. \ref{sec:entropy}) with $k_{\min}=2$ and $k_{\max}=8$. To ensure a fair comparison, the hyperparameters for the MLP backbone are retrieved from the TALENT repository\footnote{\url{https://github.com/LAMDA-Tabular/TALENT}} for each specific dataset. For datasets without pre-tuned settings, we adopt the default configuration: two hidden layers of size 384 with a dropout of 0.1, trained with a learning rate of $3 \times 10^{-4}$ and weight decay of $1 \times 10^{-5}$. The LDNs are optimized using Riemannian Adam, with its learning rate and weight decay consistent with the standard AdamW optimizer used for Euclidean parameters.

\textbf{Evaluation Metrics and Ranking.}
Our model was evaluated on all datasets with 10 repeated runs, and mean accuracy was recorded. We report the average rank computed using the Wilcoxon–Holm test \cite{demvsar2006statistical} over all accuracy results at a significance level of 0.05. It is critical to note that since ranking is a relative metric, the absolute rank of a model (e.g., HDE-Net) depends entirely on the specific model and dataset pool included in the comparison. Consequently, the ranks reported in the \textit{Performance Comparison} (Sec. \ref{sec:main_results}), \textit{Performance Comparison} (Sec. \ref{sec:robustness}) and the \textit{Mechanism Analysis and Ablation Study} (Sec. \ref{sec:ablation}) may differ due to the varying sets of HDE-Net variants involved.

\subsection{Performance Comparison}
\label{sec:main_results}

The overall performance of HDE-Net and 31 baseline models on the TALENT-tiny-core benchmark is shown in Table~\ref{tab:HDE-Net_perf}, and its ranking is summarized in Figure \ref{fig:cd_diagram}. HDE-Net achieves a leading Average Rank of \textit{4.1667}, securing the \textit{1st position} among all compared methods. 
HDE-Net exhibits a clear statistical advantage over a vast majority of deep learning models and traditional tree-based models. Notably, HDE-Net significantly outperforms the current state-of-the-art retrieval-based model, ModernNCA (Rank 7.63), and the industrial gold standard, XGBoost (Rank 7.98). This result validates our core design philosophy: by establishing a geometric isomorphism with decision trees at the representation level, HDE-Net combines the robustness of tree models with the expressive power of neural networks, achieving superior generalization without the need for complex backbones or expensive retrieval processes.

\subsection{Robustness across Feature Scenarios}
\label{sec:robustness}

To further investigate the adaptability of HDE-Net to different feature compositions, we analyze the average ranking across three specific scenarios: \textit{Num-only} (Datasets contains only numerical features), \textit{Num-Heavy} (Numerical dominant datasets), and \textit{Cat-Heavy} (Categorical dominant datasets). Since the benchmark contains only one categorical-only dataset (\texttt{ozone\_level}), it is merged into the \textit{Cat-Heavy} group. We select representative models from each paradigm for comparison: XGBoost (Tree), RealMLP (Ad-hoc Fusion), FT-Transformer (Pseudo-Alignment), and ModernNCA (Retrieval). The results are illustrated in Figure \ref{fig:scenarios}.

\noindent\textbf{Effective Numerical Representation.} In the \textit{Num-only} scenario, HDE-Net achieves a rank of 4.64, outperforming RealMLP (7.04) and FT-Transformer (13.25). 
This performance gap empirically validates that our Soft Decision Routing mechanism effectively captures the fine-grained semantics of numerical features at the value level. In contrast, standard Transformer architectures (like FTT) struggle to model pure numerical manifolds efficiently due to their reliance on coarse linear projections.

\noindent\textbf{Robustness to Feature Heterogeneity.} The results further highlight the resilience of HDE-Net against varying feature compositions. Ad-hoc Fusion methods exhibit performance inconsistency; for instance, RealMLP performs reasonably in \textit{Num-Only} (7.04) but degrades severely in \textit{Cat-Heavy} scenarios (Rank 13.86). Conversely, HDE-Net maintains consistent superiority, achieving a rank of 3.57 in \textit{Cat-Heavy} tasks, which is comparable to the tree-based XGBoost (3.43). This confirms that by embedding LDNs in the Poincaré ball, HDE-Net successfully replicates tree-like hierarchical reasoning for categorical data while retaining expressiveness for numerical features.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{GroupedPerformance.png}
    \vskip -10pt
    \caption{Comparison of average rankings across different feature scenarios. HDE-Net demonstrates superior stability and performance across all scenarios, while methods like RealMLP exhibit severe specialization bias (collapsing in categorical-dominant tasks).}
    \label{fig:scenarios}
\end{figure}

% Table generated by Excel2LaTeX from sheet 'Params'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Ablation study of HDE-Net components.} Results show that Soft Decision Routing alone (\textit{HDE-noHyp}) already surpasses previous SOTAs like ModernNCA, while Hyperbolic constraints (\textit{HDE-Net}) provide the critical boost to secure the top rank.}
  \vskip -10pt
    \begin{tabular}{lrlr}
    \toprule
    \textbf{Variant} & \textbf{Rank} & \textbf{Baseline} & \textbf{Rank} \\
    \midrule
    \textbf{HDE-Net} & \textbf{4.93} & ModernNCA & 8.70 \\
    HDE-noHyp & 7.10  & TabR  & 9.48 \\
    HDE-noRouter & 13.45 & RealMLP & 10.82 \\
          &       & FT-Transformer & 13.93 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ablation}%
  \vskip -10pt
\end{table}%

\subsection{Mechanism Analysis and Ablation Study}
\label{sec:ablation}

In this section, we conduct a detailed ablation study to quantify the contributions of HDE-Net's core components and evaluate its sensitivity to LDNs' $k$. \textit{Note on Rankings}: Average rank is a relative metric; values here differ from Section \ref{sec:main_results} because the model pool includes additional HDE variants for comparison.

\noindent\textbf{Impact of Core Mechanisms} 
To disentangle the contributions of our design choices, we evaluate two variants: \textit{HDE-noRouter}, which reverts numerical feature processing to a single hyperbolic linear mapping \cite{shimizu2020hyperbolic}, and \textit{HDE-noHyp}, which retains the routing mechanism but constrains all LDNs to Euclidean space. We compare these against leading baselines in Table \ref{tab:ablation}.

\begin{itemize}
    \item \textbf{Soft Routing Suffices for SOTA-level Performance.} A remarkable finding is that even without hyperbolic geometry, \textit{HDE-noHyp} (Rank 7.10) significantly outperforms previous state-of-the-art models, including ModernNCA (8.70) and RealMLP (10.82). This strongly validates our hypothesis: resolving the \textit{Representation Granularity Mismatch} via Soft Decision Routing is the most critical factor in tabular deep learning, outweighing the benefits of complex retrieval or ad-hoc fusion strategies.
    
    \item \textbf{Geometric Alignment via Hyperbolic Space.} While Euclidean routing is effective, introducing the hyperbolic manifold elevates the performance from Rank 7.10 to 4.93. This gain indicates that the Poincaré ball provides a more efficient way for modeling hierarchical decision structures. Unlike Euclidean space, which requires exponentially increasing capacity to approximate tree-like hierarchies, hyperbolic space preserves such structures more naturally, facilitating the modeling of hierarchical dependencies.
    
    \item \textbf{High Capacity with Low Dimensionality.} Even in its most constrained form, \textit{HDE-noRouter} (Rank 13.45) remains highly competitive with the computation-heavy FT-Transfo\-rmer (13.93). It is worth noting that HDE operates with a minimal 12-dimensional embedding and a simple MLP, whereas FT-Transformer relies on high-dimensional spaces ($d>100$) and a deep Transformer backbone. This underscores the exceptional information density and efficiency of hyperbolic representations.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{K_sensitiveness.png}
    \vskip -10pt
    \caption{Parameter sensitivity analysis across different LDN capacities ($k$). HDE-Net (Adaptive $k$) consistently outperforms all fixed-$k$ variants, proving the necessity of entropy-aware allocation.}
    \vskip -10pt
    \label{fig:sensitivity_bar}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{RouterVisual.png}
    \vskip -10pt
    \caption{Visualization of Soft Decision Router weights compared to Decision Tree split points. The intersections and zero-crossings of LDN weights closely match the tree's hard thresholds, proving the isomorphism in split logic.}
    \label{fig:router_vis}
\end{figure*}

\noindent\textbf{Efficacy of Entropy-aware Allocation}
We further analyze the parameter sensitivity of the number of LDNs ($k$). We compare HDE-Net (Adaptive $k$) against variants with fixed $k \in \{2, 4, 6, 8, 10\}$. 
The results (Fig~\ref{fig:sensitivity_bar}) exhibit a clear Arch-shaped ranking trend for fixed-capacity models ($k=2$ to $k=10$). Models with $k \le 6$ suffer from under-parameterization (insufficient decision nodes), while $k=10$ shows signs of over-parameterization, increasing the complexity burden on the MLP classifier. 
Meanwhile, HDE-Net (Rank 5.73) significantly outperforms the best fixed variant ($k=8$, Rank 7.92). This empirical gap demonstrates the superiority of our adaptation which effectively assigns decision bandwidth where it is most needed, finding a superior balance between expressiveness and efficiency that no fixed-parameter model can achieve.

\subsection{Efficiency Analysis: Complexity and Cost}
\label{sec:efficiency}

To evaluate the computational efficiency of HDE-Net, we analyze its theoretical complexity and quantitatively compare it against leading baselines. 

\noindent\textbf{Theoretical Complexity.}
Structure-based models like FT-Transfo\-rmer rely on self-attention mechanisms, resulting in a computational complexity of $\mathcal{O}(L \cdot N_{feat}^2)$, where $L$ is the number of layers and $N_{feat}$ is the number of features. Retrieval-based methods like TabR and ModernNCA require searching nearest neighbors from the training set size $N_{train}$ at inference time, leading to a complexity of $\mathcal{O}(N_{train} \cdot d)$.
In contrast, HDE-Net employs a lightweight MLP backbone. The HDE layer performs element-wise routing and aggregation, scaling linearly with the number of features and the corresponding capacity $K$ (for simplicity, here $K$ denotes both the cardinality of categorical features and the number of LDNs allocated to numerical features). Thus, the complexity of HDE-Net is strictly $\mathcal{O}(N_{feat} \cdot K + \text{MLP}_{depth})$, eliminating the quadratic bottleneck of Transformers and the retrieval overhead of neighbor-based methods.

\noindent\textbf{Quantitative Comparison.}
 We selected the \texttt{INNHotelsGroup} dat\-aset from the TALENT benchmark, which features a balanced composition of categorical (6) and numerical (11) features with 29,020 samples, providing a representative scenario for both retrieval-based and architecture-based methods. Inference time was measured on an NVIDIA RTX A4000 GPU with a batch size of 10,240 to maximize throughput and minimize I/O overhead. We compared HDE-Net against top-performing DNN baselines: FT-Transformer (Structure-based), ModernNCA (Retrieval-based), and RealMLP (Ad-hoc Fusion), using their optimal hyperparameter configurations provided by the benchmark. As detailed in Table \ref{tab:efficiency}, HDE-Net achieves a remarkable balance between efficiency and performance:
 
 \begin{table}[h]
  \centering
  \caption{\textbf{Efficiency Comparison on INNHotelsGroup.} Inference time is measured for a single forward pass of the full test set (Batch Size = 10,240). Rank refers to the average rank on the full TALENT benchmark. HDE-Net achieves the best rank with the lowest latency.}
  \vskip -10pt
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{Params (M)} & \textbf{Infer. Time (s)} & \textbf{Avg. Rank} \\
    \midrule
    FT-Transformer & 0.91  & 0.0440 & 12.55 \\
    RealMLP & 0.15  & 0.1293$^\dagger$ & 9.68 \\
    ModernNCA & 0.06  & 0.0552 & 7.63 \\
    \textbf{HDE-Net (Ours)} & 0.63  & \textbf{0.0077} & \textbf{4.17} \\
    \bottomrule
    \multicolumn{4}{l}{\footnotesize $^\dagger$ Note: High latency likely due to implementation overhead in the benchmark suite.}
    \end{tabular}%
  \label{tab:efficiency}%
\end{table}%

\begin{itemize}
    \item \textbf{Vs. Structure-based (FT-Transformer):} While HDE-Net has a comparable parameter count to FT-Transformer (0.63M vs. 0.91M), it is approximately \textit{5.7$\times$ faster} (0.0077s vs. 0.0440s). This empirically confirms that shifting complexity from the backbone (Transformer layers) to the embedding layer (HDE) significantly reduces computational latency without sacrificing accuracy.
    \item \textbf{Vs. Retrieval-based (ModernNCA):} Although ModernNCA has fewer parameters due to its non-parametric nature, its inference latency is significantly higher (0.0552s, \textit{$\approx$7.1$\times$ slower} than HDE-Net). This highlights the advantage of HDE's design: effective architectural design alone can achieve strong performance–efficiency trade-offs, even without retrieval.
    \item \textbf{Vs. Ad-hoc Fusion (RealMLP):} Theoretically, RealMLP should be efficient due to its simple architecture. However, we observed anomalously high latency (0.1293s) in the benchmark implementation. We attribute this to engineering disparities in the TALENT codebase rather than inherent algorithmic complexity. Even disregarding this anomaly, HDE-Net significantly outperforms RealMLP in ranking (4.17 vs. 9.68), justifying the overhead of the HDE layer.
\end{itemize}

In summary, HDE-Net dominates the trade-off landscape, delivering the fastest inference speed among competitive baselines while maintaining the best overall ranking.

\subsection{Qualitative Analysis: Geometric Isomorphism}
\label{sec:visualization}

To validate our core claim that HDE-Net establishes a geometric isomorphism with decision trees, we conduct a qualitative analysis focusing on split logic and hierarchical structure.

\noindent\textbf{Split Logic Alignment via Router Weights.} 
We examine whether the \textit{Soft Decision Router} reconstructs the range-splitting logic of decision trees. For a specific numerical feature $j$, we visualize all related routing weights $\bm{W}_j = \{w_{j,0}, ..., w_{j, k_j}\}$ (from Eq.~\ref{eq:router_weight}) across its entire value range. As shown in Figure \ref{fig:router_vis}, the x-axis represents sorted feature input values, and each colored line corresponds to one weight within $\bm{W}_j$. 

In \texttt{ibm-employee-performance} ($k=2$) and \texttt{company-bankrup\-tcy} ($k=4$) datasets, the intersection points of these weight curves (one LDN's influence begins to surpass another) align remarkably well with the hard split thresholds learned by a standard Decision Tree (via \textit{scikit-learn}\footnote{https://scikit-learn.org/}). Moreover, the zero-crossing points (where $w_{j,k} = 0$) also align with signify a transition in the node's contribution from negative to positive, effectively acting as a soft decision boundary. This phenomenon demonstrates that HDE-Net implicitly reconstructs the discrete range-splitting mechanism of trees while maintaining a differentiable formulation suitable for gradient-based optimization.

\noindent\textbf{Hierarchical Structure in Poincaré Space.}
We further visualize the learned distribution of HDEs to confirm their hierarchical nature. We first compute the pairwise manifold distance matrix $\mathbf{D} \in \mathbb{R}^{M \times M}$ for all $M$ categorical HDEs using the Poincaré distance formula $d_{\mathbb{B}}$ defined in Eq. (1). Then, we apply \textit{Multidimensional Scaling (MDS)} to project these hyperbolic points into a 2D Euclidean space. 
Each LDN is colored based on its \textit{Feature Importance} (Cover Rate) provided by a pre-trained XGBoost model, representing its relative level in the tree hierarchy. Note that the Soft Decision Router is not strictly equivalent to the range splits in GBDTs. Therefore, we visualize only the HDEs of categorical features. To this end, we select two categorical-dominant datasets, \texttt{ozone\_level} (36 categorical features) and \texttt{allbp} (23 categorical features), for our visualization (Fig. \ref{fig:hde_vis}). There are two findings:
\begin{figure}[h]
    \centering
    \includegraphics[width=230pt]{EmbeddingVisual.png}
    \vskip -10pt
    \caption{HDE visualized via MDS and colored by XGBoost Feature importance. The emergence of fractal structures and the center-to-boundary importance hierarchy confirm the induction of a tree-like structure.}
    \label{fig:hde_vis}
\end{figure}
\begin{itemize}
	\item \textbf{Branching Pattern Emergence}: Starting from a random initialization (Epoch 0), the HDEs progressively organize into distinct clusters with fractal-like branching patterns, mirroring the topology of decision trees. 
	\item \textbf{Hierarchy-Aware Distribution}: Highly important HDEs (darker red, higher cover rate) tend to reside near the center of the space, effectively acting as ``root-like'' nodes with broader influence. Conversely, HDEs with lower importance (lighter yellow) are pushed towards the boundary, representing ``leaf-like'' fine-grained decision regions. This spatial arrangement serves as direct evidence that the hyperbolic manifold constraint forces the model to induce a latent reasoning hierarchy equivalent to that of GBDTs.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we address the fundamental \textit{Representation Granularity Mismatch} and \textit{Geometric Mismatch} in deep tabular learning. We propose \textbf{HDE-Net}, a framework that establishes a geometric isomorphism between neural embeddings and decision trees. Central to our approach is the unification of heterogeneous features into LDNs embedded in the Hyperbolic Poincaré ball to construct HDEs. We implement this via Soft Decision Routing, which upgrades numerical modeling to fine-grained, value-level discretizations, while our Entropy-aware Capacity Allocation ensures that the number of LDNs is adaptively allocated for feature complexity.

Extensive experiments on the TALENT-tiny-core benchmark demonstrate that HDE-Net achieves SOTA (Rank 1st) performance, consistently outperforming industrial GBDTs (CatBoost, XGBoost) and advanced deep baselines (RealMLP, ModernNCA). Crucially, this performance does not come at the cost of efficiency. Unlike Transformer-based architectures that rely on heavy attention mechanisms, HDE-Net maintains the speed advantage of the lightweight MLP. 

Our findings suggest that geometric alignment with the data's inherent structure is a promising direction. We believe HDE provides a foundational geometric framework for tabular data, paving the way for future research into scalable, cross-modal foundation models.

%In this paper, we address the fundamental Dual Deficiency of deep tabular learning, representation granularity mismatch and the lack of intrinsic hierarchical geometry. By proposing \textbf{HDE-Net}, we establish a novel geometric isomorphism between neural embeddings and the robust reasoning logic of decision trees. Our framework unifies heterogeneous features into a standardized set of LDNs within the Poincaré ball model, where Soft Decision Routing elevates numerical feature modeling from a coarse feature-level projection to a fine-grained value-level discretization. 
%
%Through extensive experiments on the 30 datasets of the TALENT-tiny-core benchmark, HDE-Net achieves \textbf{SOTA (Rank 1st)} performance, consistently outperforming both the industrial gold standard GBDTs and leading tabular DNN baselines. Beyond performance metrics, our qualitative visualizations provide empirical proof that HDE-Net induces a tree-equivalent hierarchy directly within its embedding space, where important decision boundaries align with those of traditional trees. This heavy-representation, light-backbone design demonstrates that deep models can surpass tree-based systems without resorting to bloated architectures or expensive retrieval mechanisms. We believe that HDE provides a promising geometric foundation for future tabular learning research, and we plan to explore its scalability to cross modality datasets and its potential for foundation model integration in future work.

\iffalse
\section{Introduction}
ACM's consolidated article template, introduced in 2017, provides a
consistent \LaTeX\ style for use across ACM publications, and
incorporates accessibility and metadata-extraction functionality
necessary for future Digital Library endeavors. Numerous ACM and
SIG-specific \LaTeX\ templates have been examined, and their unique
features incorporated into this single new template.

If you are new to publishing with ACM, this document is a valuable
guide to the process of preparing your work for publication. If you
have published with ACM before, this document provides insight and
instruction into more recent changes to the article template.

The ``\verb|acmart|'' document class can be used to prepare articles
for any ACM publication --- conference or journal, and for any stage
of publication, from review to final ``camera-ready'' copy, to the
author's own version, with {\itshape very} few changes to the source.

\section{Template Overview}
As noted in the introduction, the ``\verb|acmart|'' document class can
be used to prepare many different kinds of documentation --- a
double-anonymous initial submission of a full-length technical paper, a
two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
journal article, a SIGCHI Extended Abstract, and more --- all by
selecting the appropriate {\itshape template style} and {\itshape
  template parameters}.

This document will explain the major features of the document
class. For further information, the {\itshape \LaTeX\ User's Guide} is
available from
\url{https://www.acm.org/publications/proceedings-template}.

\subsection{Template Styles}

The primary parameter given to the ``\verb|acmart|'' document class is
the {\itshape template style} which corresponds to the kind of publication
or SIG publishing the work. This parameter is enclosed in square
brackets and is a part of the {\verb|documentclass|} command:
\begin{verbatim}
  \documentclass[STYLE]{acmart}
\end{verbatim}

Journals use one of three template styles. All but three ACM journals
use the {\verb|acmsmall|} template style:
\begin{itemize}
\item {\texttt{acmsmall}}: The default journal template style.
\item {\texttt{acmlarge}}: Used by JOCCH and TAP.
\item {\texttt{acmtog}}: Used by TOG.
\end{itemize}

The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
\begin{itemize}
\item {\texttt{sigconf}}: The default proceedings template style.
\item{\texttt{sigchi}}: Used for SIGCHI conference articles.
\item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
\end{itemize}

\subsection{Template Parameters}

In addition to specifying the {\itshape template style} to be used in
formatting your work, there are a number of {\itshape template parameters}
which modify some part of the applied template style. A complete list
of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

Frequently-used parameters, or combinations of parameters, include:
\begin{itemize}
\item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
  conference submission. Anonymizes the work and includes line
  numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
  submission's unique ID on each page of the work.
\item{\texttt{authorversion}}: Produces a version of the work suitable
  for posting by the author.
\item{\texttt{screen}}: Produces colored hyperlinks.
\end{itemize}

This document uses the following string as the first command in the
source file:
\begin{verbatim}
\documentclass[sigconf,authordraft]{acmart}
\end{verbatim}

\section{Modifications}

Modifying the template --- including but not limited to: adjusting
margins, typeface sizes, line spacing, paragraph and list definitions,
and the use of the \verb|\vspace| command to manually adjust the
vertical spacing between elements of your work --- is not allowed.

{\bfseries Your document will be returned to you for revision if
  modifications are discovered.}

\section{Typefaces}

The ``\verb|acmart|'' document class requires the use of the
``Libertine'' typeface family. Your \TeX\ installation should include
this set of packages. Please do not substitute other typefaces. The
``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
as they will override the built-in typeface families.

\section{Title Information}

The title of your work should use capital letters appropriately -
\url{https://capitalizemytitle.com/} has useful rules for
capitalization. Use the {\verb|title|} command to define the title of
your work. If your work has a subtitle, define it with the
{\verb|subtitle|} command.  Do not insert line breaks in your title.

If your title is lengthy, you must define a short version to be used
in the page headers, to prevent overlapping text. The \verb|title|
command has a ``short title'' parameter:
\begin{verbatim}
  \title[short title]{full title}
\end{verbatim}

\section{Authors and Affiliations}

Each author must be defined separately for accurate metadata
identification.  As an exception, multiple authors may share one
affiliation. Authors' names should not be abbreviated; use full first
names wherever possible. Include authors' e-mail addresses whenever
possible.

Grouping authors' names or e-mail addresses, or providing an ``e-mail
alias,'' as shown below, is not acceptable:
\begin{verbatim}
  \author{Brooke Aster, David Mehldau}
  \email{dave,judy,steve@university.edu}
  \email{firstname.lastname@phillips.org}
\end{verbatim}

The \verb|authornote| and \verb|authornotemark| commands allow a note
to apply to multiple authors --- for example, if the first two authors
of an article contributed equally to the work.

If your author list is lengthy, you must define a shortened version of
the list of authors to be used in the page headers, to prevent
overlapping text. The following command should be placed just after
the last \verb|\author{}| definition:
\begin{verbatim}
  \renewcommand{\shortauthors}{McCartney, et al.}
\end{verbatim}
Omitting this command will force the use of a concatenated list of all
of the authors' names, which may result in overlapping text in the
page headers.

The article template's documentation, available at
\url{https://www.acm.org/publications/proceedings-template}, has a
complete explanation of these commands and tips for their effective
use.

Note that authors' addresses are mandatory for journal articles.

\section{Rights Information}

Authors of any work published by ACM will need to complete a rights
form. Depending on the kind of work, and the rights management choice
made by the author, this may be copyright transfer, permission,
license, or an OA (open access) agreement.

Regardless of the rights management choice, the author will receive a
copy of the completed rights form once it has been submitted. This
form contains \LaTeX\ commands that must be copied into the source
document. When the document source is compiled, these commands and
their parameters add formatted text to several areas of the final
document:
\begin{itemize}
\item the ``ACM Reference Format'' text on the first page.
\item the ``rights management'' text on the first page.
\item the conference information in the page header(s).
\end{itemize}

Rights information is unique to the work; if you are preparing several
works for an event, make sure to use the correct set of commands with
each of the works.

The ACM Reference Format text is required for all articles over one
page in length, and is optional for one-page articles (abstracts).

\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).

\section{Sectioning Commands}

Your work should use standard \LaTeX\ sectioning commands:
\verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
\verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
\verb|\subsusection| should be numbered; do not remove the numbering
from the commands.

Simulating a sectioning command by setting the first word or words of
a paragraph in boldface or italicized text is {\bfseries not allowed.}

Below are examples of sectioning commands.

\subsection{Subsection}
\label{sec:subsection}

This is a subsection.

\subsubsection{Subsubsection}
\label{sec:subsubsection}

This is a subsubsection.

\paragraph{Paragraph}

This is a paragraph.

\subparagraph{Subparagraph}

This is a subparagraph.

\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
high-quality tables.

Table captions are placed {\itshape above} the table.

Because tables cannot be split across pages, the best placement for
them is typically the top of the page nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and the
table caption.  The contents of the table itself must go in the
\textbf{tabular} environment, to be aligned properly in rows and
columns, with the desired horizontal and vertical rules.  Again,
detailed instructions on \textbf{tabular} material are found in the
\textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed output of
this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}

Always use midrule to separate table header rows from data rows, and
use it only for this purpose. This enables assistive technologies to
recognise table headers and support their users in navigating tables
more easily.

\section{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of the three are
discussed in the next sections.

\subsection{Inline (In-text) Equations}
A formula that appears in the running text is called an inline or
in-text formula.  It is produced by the \textbf{math} environment,
which can be invoked with the usual
\texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
and structures, from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
examples of in-text equations in context. Notice how this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols and
structures available in \LaTeX\@; this section will just give a couple
of examples of display equations in context.  First, consider the
equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or
more images can be placed within a figure. If your figure contains
third-party material, you must clearly identify it as such, as shown
in the example below.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sample-franklin}
  \caption{1907 Franklin Model D roadster. Photograph by Harris \&
    Ewing, Inc. [Public domain], via Wikimedia
    Commons. (\url{https://goo.gl/VLCRBB}).}
  \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Your figures should contain a caption which describes the figure to
the reader.

Figure captions are placed {\itshape below} the figure.

Every figure should also have a figure description unless it is purely
decorative. These descriptions convey what’s in the image to someone
who cannot see it. They are also used by search engine crawlers for
indexing images, and when images cannot be loaded.

A figure description must be unformatted plain text less than 2000
characters long (including spaces).  {\bfseries Figure descriptions
  should not repeat the figure caption – their purpose is to capture
  important information that is not already provided in the caption or
  the main text of the paper.} For figures that convey important and
complex new information, a short text description may not be
adequate. More complex alternative descriptions can be placed in an
appendix and referenced in a short figure description. For example,
provide a data table capturing the information in a bar chart, or a
structured list representing a graph.  For additional information
regarding how best to write figure descriptions and why doing this is
so important, please see
\url{https://www.acm.org/publications/taps/describing-figures/}.

\subsection{The ``Teaser Figure''}

A ``teaser figure'' is an image, or set of images in one figure, that
are placed after all author and affiliation information, and before
the body of the article, spanning the page. If you wish to have such a
figure in your article, place the command immediately before the
\verb|\maketitle| command:
\begin{verbatim}
  \begin{teaserfigure}
    \includegraphics[width=\textwidth]{sampleteaser}
    \caption{figure caption}
    \Description{figure description}
  \end{teaserfigure}
\end{verbatim}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}


  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  presentation~\cite{Reiser2014}. An article under
  review~\cite{Baggett2025}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
  Artifacts: \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

Identification of funding sources and other support, and thanks to
individuals and groups that assisted in the research and the
preparation of the work should be included in an acknowledgment
section, which is placed just before the reference section in your
document.

This section has a special environment:
\begin{verbatim}
  \begin{acks}
  ...
  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{description}
\item[\texttt{sidebar}:]  Place formatted text in the margin.
\item[\texttt{marginfigure}:] Place a figure in the margin.
\item[\texttt{margintable}:] Place a table in the margin.
\end{description}
\fi

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{unsrt}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix
\section{Statistics of the Datasets}
\label{app:datasets}
Table \ref{tab:datasets} presents detailed statistics of the datasets in the TALENT-tiny-core classification benchmark. Class denotes the number of target classes, Cat the number of categorical features, Num the number of numerical features, and Size the dataset size. All statistics are obtained from the official TALENT GitHub repository.
\begin{table}[htbp]
\small  \centering  \caption{Detailed statistics of the datasets in the TALENT-tiny-core classification benchmark}
  	\scalebox{1.0}{    \begin{tabular}{rrrrr}    \toprule    \multicolumn{1}{l}{\textbf{name}} & \multicolumn{1}{l}{\textbf{Class}} & \multicolumn{1}{l}{\textbf{Cat}} & \multicolumn{1}{l}{\textbf{Num}} & \multicolumn{1}{l}{\textbf{Size}} \\    \midrule    \multicolumn{1}{l}{ada} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{48} & \multicolumn{1}{c}{3317} \\    \multicolumn{1}{l}{airlines} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{1600} \\    \multicolumn{1}{l}{allbp} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{3017} \\    \multicolumn{1}{l}{ASP-POTASSCO} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{140} & \multicolumn{1}{c}{1035} \\    \multicolumn{1}{l}{autoUniv-au7-1100} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{880} \\    \multicolumn{1}{l}{company\_bankruptcy} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{93} & \multicolumn{1}{c}{5455} \\    \multicolumn{1}{l}{eucalyptus} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{14} & \multicolumn{1}{c}{588} \\    \multicolumn{1}{l}{Gender\_Gap\_in\_Spanish} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{3796} \\    \multicolumn{1}{l}{hill-valley} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{969} \\    \multicolumn{1}{l}{house\_16H} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{16} & \multicolumn{1}{c}{10790} \\    \multicolumn{1}{l}{ibm-employee-performance} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{1176} \\    \multicolumn{1}{l}{INNHotelsGroup} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{29020} \\    \multicolumn{1}{l}{internet\_firewall} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{52425} \\    \multicolumn{1}{l}{jasmine} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{136} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{2387} \\    \multicolumn{1}{l}{jungle\_chess\_2pcs} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{35855} \\    \multicolumn{1}{l}{law-school-admission} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{16640} \\    \multicolumn{1}{l}{microaggregation2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{20} & \multicolumn{1}{c}{16000} \\    \multicolumn{1}{l}{national-longitudinal} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{3926} \\    \multicolumn{1}{l}{okcupid\_stem} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{21341} \\    \multicolumn{1}{l}{online\_shoppers} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{9864} \\    \multicolumn{1}{l}{ozone\_level} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{36} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{2028} \\    \multicolumn{1}{l}{pc4} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{37} & \multicolumn{1}{c}{1166} \\    \multicolumn{1}{l}{PhishingWebsites} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{30} & \multicolumn{1}{c}{8844} \\    \multicolumn{1}{l}{rice\_cammeo\_\&\_osmancik} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{3048} \\    \multicolumn{1}{l}{shill-bidding} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5056} \\    \multicolumn{1}{l}{Shipping} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{8799} \\    \multicolumn{1}{l}{statlog} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{800} \\    \multicolumn{1}{l}{thyroid} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{5760} \\    \multicolumn{1}{l}{waveform} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{4000} \\    \multicolumn{1}{l}{wine-quality-red} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{1279} \\    \midrule          &       &       &       &  \\    \end{tabular}%
    }  \label{tab:datasets}%\end{table}%

\section{Models for Comparison}
\label{app:comp_models}
Table \ref{tab:comp_models} lists all models included in the ranking, with results provided by the official TALENT project. Note that some recent models, such as TabM and TabPFN v2, are excluded because their official benchmark results on TALENT have not yet been released, although their code has been integrated into the framework. We report full dataset-level results for HDE-Net to facilitate future comparisons.
\begin{table}[]
\caption{Models for Comparison}
\label{tab:comp_models}
\begin{tabular}{|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Traditional \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}dummy, LogReg, NCM, NaiveBayes,\\ knn, svm, xgboost, catboost,\\ RandomForest, lightgbm\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Deep Learning \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}tabpfn, mlp, resnet, node, switchtab,\\ tabnet, tabcaps, tangos, danets, ftt,\\ autoint, dcn2, snn, tabtransformer,\\ ptarl, grownet, tabr, modernNCA,\\ mlp\_plr, realmlp, excelformer\end{tabular} \\ \hline
\end{tabular}
\end{table}

%\section{Research Methods}
%
%\subsection{Part One}
%
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
%malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
%sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
%vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
%lacinia dolor. Integer ultricies commodo sem nec semper.
%
%\subsection{Part Two}
%
%Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
%ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
%ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
%eros. Vivamus non purus placerat, scelerisque diam eu, cursus
%ante. Etiam aliquam tortor auctor efficitur mattis.
%
%\section{Online Resources}
%
%Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
%pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
%enim maximus. Vestibulum gravida massa ut felis suscipit
%congue. Quisque mattis elit a risus ultrices commodo venenatis eget
%dui. Etiam sagittis eleifend elementum.
%
%Nam interdum magna at lectus dignissim, ac dignissim lorem
%rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
