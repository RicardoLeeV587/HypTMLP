%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%\documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf,anonymous,review,nonacm]{acmart}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{threeparttable}
\usepackage{bm}

\usepackage{url}
\usepackage{hyperref} % 用于制作可点击的超链接

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Manifold Constrained Tabular Deep Neural Networks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract} 

While Deep Neural Networks (DNNs) have transformed perception tasks, Gradient-Boosted Decision Trees (GBDTs) remain the dominant baseline for tabular data. Their success stems from the rule-driven nature of tabular prediction, where relationships are often governed by local, condition-triggered patterns that require conditional interactions and non-uniform model complexity. Tree-based models naturally capture such structures through hierarchical space partitioning. In contrast, Tabular DNNs rely on Euclidean representations that assume smooth variations and semantic locality, which often contradict the rule-partitioned nature of tabular data modeling. This geometric incompatibility creates a fundamental representational bottleneck that hinders the expressive power of DNNs on tabular tasks.
To address this, we propose \textit{HDE-Net}, a manifold-constrained DNN that unifies heterogeneous feature conditions into a symbolic set of Latent Decision Nodes (LDNs) and embeds them as \textit{Hyperbolic Decision Embeddings} (HDEs) in the Poincaré ball, a continuous analogue of tree structures. For numerical features, we introduce a \textit{Soft Decision Routing} mechanism that approximates range-based local rules in a differentiable manner, while an \textit{Entropy-aware Capacity Allocation} algorithm adaptively allocates capacity to LDNs. On the TALENT-tiny-core classification benchmark (30 datasets), HDE-Net achieves the \textit{1st average rank} among 31 competitive baselines, outperforming both industrial GBDTs and state-of-the-art Tabular DNNs, while remaining highly efficient.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010342.10010343</concept_id>
       <concept_desc>Computing methodologies~Modeling methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Modeling methodologies}

%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>00000000.0000000.0000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Tabular Representation Learning, Geometric Deep Learning, Deep Tabular Learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

Despite the success of Deep Neural Networks (DNNs) in perceptual domains, tabular data remains the cornerstone of critical real-world applications, from financial risk assessment to healthcare diagnostics \cite{guo2017deepfm, buczak2015survey, hyland2020early, liu2025talent}. Unlike perceptual data (images, text) characterized by homogeneous semantic units and spatial/sequential invariance, tabular data is inherently heterogeneous and governed by discrete, condition-triggered rules. Consequently, Gradient-Boosted Decision Trees (GBDTs) such as XGBoost \cite{chen2016xgboost} and CatBoost \cite{prokhorenkova2018catboost} remain the dominant baselines \cite{liu2025talent}.

The effectiveness of GBDTs stems from their alignment with this rule-oriented structure. They perform task-driven space partitioning, recursively decomposing the feature space into a hierarchy of symbolic decision rules (e.g., $\bm{x} > 19$ or $\bm{x} = \text{male}$). In contrast, standard Tabular DNNs operate under Euclidean assumptions of smoothness and continuous manifolds \cite{arik2021tabnet, huang2020tabtransformer}. This creates a fundamental \textit{geometric incompatibility}: the flat geometry of Euclidean space is poorly suited to represent the exponential branching structure induced by hierarchical decision rules. As a result, neural models often require deep or complex architectures to approximate relatively simple rule-based relationships.

\begin{table}[htbp]
\small
  \centering
%  \vskip -8pt
  \caption{Performance breakdown on the TALENT-tiny core classification benchmark by feature composition. \textit{Overall} (All datasets), \textit{Num-Only} (Datasets contain only numerical features), \textit{Num-Heavy} (Numerical dominant datasets), and \textit{Cat-Heavy} (Categorical dominant datasets). Values represent Average Rank (lower is better). Parentheses indicate rank shifts relative to the \textbf{Overall} performance (\textcolor{green}{Green/-}: Improved Rank; \textcolor{red}{Red/+}: Worsened Rank).}
%  \vskip -5pt
  	\scalebox{0.8}{
    \begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Models}} & \textbf{Overall} & \textbf{Num-Only} & \textbf{Num-Heavy} & \textbf{Cat-Heavy} \\
    \midrule
    \multicolumn{1}{c}{\multirow{3}[2]{*}{\makecell*[c]{Tree\\Models}}} 
          & XGBoost & 7.4  & 10.5 \textcolor{red}{(+3.1)} & 5.9 \textcolor{green}{(-1.5)} & 3.0 \textcolor{green}{(-4.3)} \\
          & LightGBM & 8.4  & 11.0 \textcolor{red}{(+2.6)} & 6.5 \textcolor{green}{(-1.9)} & 5.5 \textcolor{green}{(-2.8)} \\
          & CatBoost & 8.6  & 12.3 \textcolor{red}{(+3.7)} & 6.4 \textcolor{green}{(-2.2)} & 3.9 \textcolor{green}{(-4.7)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Ad-hoc\\Fusion}}} 
          & RealMLP & 8.9  & 6.4 \textcolor{green}{(-2.5)} & 9.7 \textcolor{red}{(+0.8)} & 12.9 \textcolor{red}{(+4.0)} \\
          & MLP   & 14.9 & 10.1 \textcolor{green}{(-4.8)} & 17.8 \textcolor{red}{(+2.8)} & 21.0 \textcolor{red}{(+6.1)} \\
    \midrule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\makecell*[tc]{Pseudo\\Alignment}}} 
          & FT-Transformer & 11.6 & 12.3 \textcolor{red}{(+0.6)} & 11.3 \textcolor{green}{(-0.3)} & 10.7 \textcolor{green}{(-0.9)} \\
          & ExcelFormer & 13.9 & 14.7 \textcolor{red}{(+0.8)} & 13.3 \textcolor{green}{(-0.6)} & 13.2 \textcolor{green}{(-0.7)} \\
    \bottomrule
    \end{tabular}%
    }
%    \vskip -10pt
  \label{tab:motivation}%
\end{table}%

To adapt DNNs for tabular data, prior work has explored various feature processing paradigms, yet many struggle with a \textit{representation granularity mismatch}. As shown in Table \ref{tab:motivation}, existing approaches exhibit noticeable performance issues across different feature compositions. 
\textit{Ad-hoc fusion} methods \cite{gorishniy2022embeddings, holzmuller2025realmlp} apply separate processing pipelines for numerical and categorical features without a unified semantic representation. While effective on numerical-dominant datasets (even the simplest MLP can beat Tree models), their performance degrades on categorical-heavy scenarios. 
\textit{Pseudo-alignment} methods \cite{song2019autoint, gorishniy2021revisiting, chen2023excelformer, wang2021dcn} project all features into a shared embedding space. However, this alignment is primarily dimensional rather than semantic. Categorical features receive fine-grained value-level embeddings, while numerical features are restricted to rigid feature-level linear space. Without mechanisms for local range-based partitioning, numerical features cannot achieve comparable semantic granularity. As a result, these models often rely on heavy backbones to compensate for representational limitations, leading to moderate performance and increased computational cost.

To address this geometric incompatibility, hyperbolic geometry provides a natural mathematical foundation. The Poincaré ball features negative curvature and exponential volume growth, making it a continuous analogue of tree structures \cite{nickel2017poincare}. While hyperbolic neural networks have shown success in graph learning by embedding explicit hierarchical structures \cite{peng2021hyperbolic}, their potential for generic tabular learning remains underexplored. A key challenge is that tabular data lacks explicit graph topology, requiring a mechanism to first unify the modeling unit (i.e., feature conditions) before constructing hyperbolic representations.

In this work, we propose \textbf{HDE-Net}, a manifold-constrained framework that establishes a geometric isomorphism between tree-structured rules and neural representations. 
To realize this, we first introduce the concept of \textbf{Latent Decision Nodes (LDNs)}: a unified modeling unit that abstracts heterogeneous feature conditions into discrete logical atoms. For numerical features, we propose \textbf{Soft Decision Routing (SDR)} to dynamically decompose continuous values into multiple LDNs, simulating the range-based split logic of trees. 
These unified LDNs are then embedded into the Poincaré ball to form \textbf{Hyperbolic Decision Embeddings (HDEs)}, where the negative curvature naturally induces a hierarchical organization. 
Finally, to balance model complexity with expressiveness, we incorporate an \textbf{entropy-aware capacity allocation} mechanism that adaptively assigns the number of LDNs based on feature information density.

Our main contributions are summarized as follows:
\begin{itemize}
    \item \textbf{A Unified Decision-Centric Framework.} 
	We introduce \textit{Latent Decision Nodes (LDNs)} as a fundamental modeling unit, aligning categorical values and numerical ranges within a shared semantic space. 
	To enable this, for numerical features, we devise \textit{Soft Decision Routing} for differentiable range-discretization and an \textit{Entropy-aware Capacity Allocation} strategy to adaptively balance granularity and complexity.

    \item \textbf{Manifold-Constrained Tabular Paradigm.}
    We propose \textit{Hyperbolic Decision Embedding (HDE)}, which represents LDNs in the Poincaré ball, forming a continuous analogue of tree-structured reasoning. 
    Built upon HDE, we develop \textbf{HDE-Net}, a manifold-constrained tabular DNN that naturally induces hierarchical decision structures within neural representations.

    \item \textbf{State-of-the-Art Performance.}
    Extensive experiments on the TALENT-tiny-core benchmark (30 datasets) demonstrate that HDE-Net achieves the \textbf{1st average rank}, outperforming both industrial GBDT models and state-of-the-art Tabular DNNs while maintaining competitive efficiency.

    \item \textbf{Geometric Visualization and Empirical Validation.}
    We design dedicated visualization analyses that reveal the geometric alignment between HDE-Net representations and decision tree structures, validating the geometric isomorphism hypothesis.
\end{itemize}

The rest of this paper is organized as follows. Section \ref{sec:related_work} reviews related work of current Tabular DNNs and Hyperbolic DNNs. Section \ref{sec:preliminaries} provides preliminaries on hyperbolic geometry and key operators. Section \ref{sec:method} details the HDE-Net. Section \ref{sec:exp} presents experimental results and analysis, followed by conclusions in Section \ref{sec:conclusion}.

\section{Related Work}
\label{sec:related_work}

\subsection{Euclidean-based Deep Tabular Paradigms}
Existing Deep Tabular Learning methods typically map heterogeneous features into Euclidean representations, implicitly assuming smooth manifolds. We categorize them based on their handling of feature heterogeneity:

\noindent\textbf{Hybrid and Fusion Architectures.} 
Methods like Wide\&Deep \cite{cheng2016wide}, TabNet \cite{arik2021tabnet}, TabTransformer \cite{huang2020tabtransformer}, MLP-PLR \cite{gorishniy2022embeddings} and RealMLP \cite{holzmuller2025realmlp} acknowledge feature differences by applying distinct pipelines (e.g., dense layers for numerical, embeddings for categorical) before fusion. While effective for specific feature types, they lack a unified semantic space. The fusion typically occurs in the final residual blocks via linear concatenation, where the flat geometry struggles to represent the hierarchical interactions inherent in rule-based tasks without excessive depth.

\noindent\textbf{Tokenization and Pseudo-Alignment.} 
Inspired by NLP, models like AutoInt \cite{song2019autoint}, FT-Transformer \cite{gorishniy2021revisiting}, DCNv2 \cite{wang2021dcn}, Excelformer \cite{chen2023excelformer} and TabM \cite{gorishniy2024tabm} enforce dimensional alignment, projecting all features into a shared Euclidean embedding space. While this enables the use of powerful Transformer backbones, the alignment is often superficial. Numerical features are typically processed via rigid feature-level linear projections ($x \cdot W + b$), which fail to capture the fine-grained, range-based split logic required for tabular reasoning. Consequently, these models often rely on heavy attention mechanisms or complex ensemble backbones to compensate for the lack of intrinsic structural alignment in the embedding layer.

\noindent\textbf{Retrieval and Prior-based Methods.} 
To overcome the limitations of parametric DNNs, methods like TabR \cite{gorishniy2023tabr} and ModernNCA \cite{ye2024revisiting} augment input representations by retrieving similar samples, while TabPFN \cite{hollmann2022tabpfn} leverages priors from synthetic datasets. Although these approaches achieve strong performance, they bypass the core representational challenge by relying on external memory or extensive pre-training, resulting in vulnerability to out-of-distribution (OOD) shifts, high inference latency and scalability bottlenecks.

\subsection{Hyperbolic Geometry and Tree Alignment}
\label{sec:rel_hyperbolic}

\noindent\textbf{Hyperbolic Representation Learning.}
Hyperbolic space is mathematically isomorphic to a continuous tree due to its exponential volume growth \cite{nickel2017poincare}. This property has driven a lot of representation learning in domains with explicit hierarchical structures, such as Graph Learning \cite{chami2019hyperbolic, dai2021hyperbolic}, Computer Vision \cite{lensink2022fully}, and Multimodal Learning (e.g., MERU \cite{desai2023hyperbolic}). However, tabular data lacks explicit topology, posing a challenge for direct application: the hierarchical decision structure must be induced implicitly from flat feature vectors.

\noindent\textbf{Geometric Tabular Models.}
Recent studies have attempted to extend traditional machine learning to non-Euclidean geometries. HyperDT \cite{chlenski2023fast} generalizes decision trees to hyperbolic space, and PXgboost \cite{suganthan2025euclidean} extends gradient boosting to the Poincaré ball. These works validate the fundamental hypothesis that hyperbolic geometry aligns with tree-based splitting logic. However, they operate as traditional classifiers. They lack the end-to-end representation learning capabilities of DNNs and lack the capacity for complex feature interaction modeling prevalent in deep tabular architectures.

\noindent\textbf{Bridging the Gap.} 
HDE-Net represents a pioneering effort to bridge these two worlds. Unlike Euclidean DNNs that struggle with structural mismatch, and unlike shallow hyperbolic ML that lacks feature learning, HDE-Net abstracts heterogeneous features into LDNs and learns their hierarchical organization within the Poincaré ball in a fully differentiable, end-to-end manner.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{Arch.png} 
  \vskip -10pt
  \caption {\textbf{Geometric Isomorphism between Decision Trees and Hyperbolic Decision Embedding (HDE).} 
\textbf{(Left)} Example DT for mood disorder prediction. 
\textbf{(Middle)} Latent Decision Nodes (LDNs) embedded in the Poincaré ball: categorical values (e.g., \textcolor{red}{Male}, \textcolor{teal}{Female}) map directly to LDNs, while numerical features (e.g., Age) are softly routed to multiple LDNs, simulating the range-based splitting logic of trees.
\textbf{(Right)} HDE-Net inference: hyperbolic embeddings (\textit{solid}) are mapped to the Euclidean tangent space (\textit{dashed}) via $\log_{\mathbf{0}}(\cdot)$ for efficient weighted aggregation (just for numerical features) and MLP classification.}
  \label{fig:arch}
  \vskip -10pt
\end{figure*}

\section{Preliminaries}
\label{sec:preliminaries}

\textbf{Problem Formulation.} 
We consider a supervised tabular learning task with a dataset $\mathcal{D} = \{(\bm{x}_i, \bm{y}_i)\}_{i=1}^N$. Each input instance $\bm{x}$ consists of heterogeneous parts: numerical features $\bm{x}^{\text{num}} \in \mathbb{R}$ and categorical features $\bm{x}^{\text{cat}} \in \mathbb{C}$, where $\mathbb{C}$ denotes the category domain. Our goal is to learn a function $f(\bm{x})$ that predicts the target $\bm{y}$. As discussed, we aim to unify the representation of these heterogeneous features into a standard set of feature conditions, effectively bridging the gap between Tabular DNN and hierarchical rule-based system.

\noindent\textbf{Poincaré Ball Model.} 
Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature. We utilize the $d$ dimensional Poincaré ball model $(\mathbb{B}_c^d, g^{\mathbb{B}})$, defined as the manifold $\mathbb{B}_c^d = \{ \bm{u} \in \mathbb{R}^d : c\| \bm{u} \|^2 < 1 \}$, where $c$ is the curvature coefficient. The induced distance between two points $\bm{u}_1, \bm{u}_2 \in \mathbb{B}_c^d$ is given by:
\begin{equation}
    d_{\mathbb{B}}(\bm{u}_1, \bm{u}_2) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c} \| -\bm{u}_1 \oplus_c \bm{u}_2 \|),
    \label{eq:dis}
\end{equation}
where $\oplus_c$ denotes the Möbius addition \cite{ungar2008analytic}. A key property of this space is that its volume grows exponentially with the radius. This aligns perfectly with the exponential growth of nodes in a decision tree as depth increases, making it an ideal continuous analogue for hierarchical tree structures \cite{nickel2017poincare}.

\noindent\textbf{Hyperbolic Embedding: Poincaré vs. Lorentz.} 
While the Lorentz model is often preferred for numerical stability in complex GNNs \cite{yang2022hyperbolic}, HDE avoids complex manifold operations (e.g., inter-node addition) and frequent Euclidean-Hyperbolic projections. We therefore select the Poincaré ball for its conformal property to Euclidean space (angle-preserving) \cite{ganea2018hyperbolic}. This allows us to intuitively encode hierarchy via the norm (depth) and logical affinity via angles (sibling relations), achieving a direct geometric isomorphism with decision trees.

\noindent\textbf{Tangent Space Projection.} 
To integrate hyperbolic embeddings with Euclidean neural networks (i.e., the MLP backbone), we utilize the \textit{Logarithmic Map} $\log_{\mathbf{0}}(\cdot)$ to project points from the manifold to the Euclidean tangent space at the origin $\mathcal{T}_{\mathbf{0}}\mathbb{B}_c^d$:
\begin{equation}
    \log_{\mathbf{0}}(\bm{u}) = \frac{2}{\sqrt{c}} \text{arctanh}(\sqrt{c}\|\bm{u}\|) \frac{\bm{u}}{\|\bm{u}\|}.
\end{equation}
In HDE, we directly initialize LDNs on the manifold and use $\log_{\mathbf{0}}(\cdot)$ as the bridge to the Euclidean predictor, enabling efficient weighted aggregation in vector space.

\section{Methodology}
\label{sec:method}

The HDE-Net (Fig~\ref{fig:arch}) consists of three key stages: 
(1) \textit{Symbolic Unification}, where heterogeneous features are mapped to a standardized set of Latent Decision Nodes (LDNs) representing discrete logical atoms to mimic tree model's feature condition;
(2) \textit{Hyperbolic Decision Embedding (HDE)}, where LDNs representation are initialized and optimized on the Poincaré ball to enforce implicit hierarchical constraints; 
(3) \textit{Tangent Projection \& Prediction}, where embeddings are projected to the Euclidean tangent space for efficient aggregation and classification. 
Within the Value-level Unification stage, we introduce two novel mechanisms specifically for numerical features: \textit{Entropy-Aware Capacity Allocation} to determine the number of LDNs, and \textit{Soft Decision Routing} to mimic the task-driven partitioning logic of GBDTs.

\subsection{Entropy-Aware Capacity Allocation}
\label{sec:entropy}
Tabular data requires non-uniform complexity allocation for each feature. Assigning a fixed number ($k$) of LDNs to every numerical feature leads to suboptimal resource allocation. Simple features may suffer from over-parameterization, while complex ones face under-parameterization. To address this, we propose the \textit{Entropy-aware Capacity Allocation} mechanism. We dynamically determine $k_j$ for each numerical feature $\bm{x}_j^{num}$ based on Shannon entropy $H(x_j^{num})$ and effective sample size $N_{eff}$ (no missing values). Here $j$ denotes a specific numerical feature. This ensures that the model allocates more decision bandwidth to features with richer information, akin to how decision trees grow deeper for critical features. The detailed procedure is outlined in Algorithm \ref{alg:entropy}.

\subsection{Soft Decision Routing}
\label{sec:sdr}
To replicate the space partitioning mechanism of GBDTs on numerical features, we introduce Soft Decision Routing to achieve a differentiable approximation of range-based splitting. 
Given an numerical feature input $\bm{x}^{num}_j$, we employ a learnable gating mechanism, termed \textit{Soft Decision Routing}, to compute $k_j$ weights to couple with $k_j$ LDNs. Specifically, we define the routing weights $\bm{W}_j \in \mathbb{R}^{k_j}$ as:
\begin{equation}
    \bm{W}_j = \boldsymbol{a}_j x^{num}_j + \boldsymbol{b}_j, \quad \boldsymbol{W}_j \in \mathbb{R}^{k_j}
    \label{eq:router_weight}
\end{equation}
where $\boldsymbol{a}, \boldsymbol{b}$ are learnable parameters. Taking Figure~\ref{fig:arch} as an example, $k$ and $\bm{W}$ are calculated for the "Age" column, and $j$ denotes the index of "Age" feature in numerical feature list. 
Unlike rigid feature-level linear projections, this mechanism computes value-dependent weights, effectively performing a soft coupling rather than a hard selection. We then use $\bm{W}_j$ for weighted aggregation without a softmax or sigmoid activation. This design allows for flexible intensity scaling, preserving the magnitude information of the input. Meanwhile, it enables non-linear expressive capacity through the intersections and zero-crossings of weight trajectories: As the input value crosses these points, the dominant/positive/negative contribution flips between different LDNs. This interaction effectively induces discrete value thresholds, allowing the model to simulate the `if-then' split logic of trees within a continuous, differentiable framework (See Fig~\ref{fig:router_vis}).

For categorical features $\bm{x}^{cat}$, the routing is deterministic, selecting the unique LDN corresponding to the category value. Consequently, categorical and numerical features are aligned in representational granularity at the value-level.

\begin{algorithm}[tbp]
\caption{Entropy-aware Capacity Allocation}
\label{alg:entropy}
\begin{algorithmic}[1]
\REQUIRE Dataset $\mathcal{D}$, Feature set $\bm{J}^{num}$, Feature input $\bm{x}^{num}$, Max capacity $k_{max}$, Min capacity $k_{min}$, 
\ENSURE Capacity map $\bm{K} = \{k_1, k_2, ..., k_j\}$
\FOR{each feature $j \in \bm{J}^{num}$}
    \STATE Calculate Shannon entropy $H(\bm{x}^{num}_j)$
    \STATE Count non-missing values $N_{eff}$
    \STATE Derive candidate capacity from entropy: $k_{ent} \leftarrow \max(k_{min}, \lfloor 2^{H(\bm{x}^{num}_j)} \rfloor)$
    \STATE Derive candidate capacity from samples: $k_{spl} \leftarrow \max(k_{min}, \lfloor \log_2(N_{eff}) \rfloor)$
    \STATE Determine final capacity: $k_j \leftarrow \min(k_{max}, k_{ent}, k_{spl})$
    \STATE Store $k_j$ in $\bm{K}$
\ENDFOR
\RETURN $\bm{K}$
\end{algorithmic}
\end{algorithm}
%\vskip -10pt

\subsection{LDNs in Hyperbolic Space}
Having unified heterogeneous features into LDNs, we now proceed to establish a geometric isomorphism with decision trees.
Let $\mathcal{M} = \mathbb{B}_c^d$ be the $d$-dimensional Poincaré ball with curvature $c$. We initialize and optimize the embeddings (HDE) $\bm{Z}$ for all LDNs (numerical and categorical) directly on this manifold $\mathcal{M}$.
The core motivation is that hyperbolic geometry serves as a continuous analogue of discrete trees. The negative curvature naturally accommodates the exponential growth of decision paths, forcing the LDNs to self-organize into a hierarchical structure during training: general symbolic concepts (root-like) naturally cluster near the origin, while fine-grained partitions (leaf-like) gravitate towards the boundary. This allows HDE-Net to induce a latent reasoning hierarchy equivalent to tree-structured rule systems purely through gradient-based optimization.


\subsection{Tangent Projection and Feature Aggregation}
To enable efficient inference and compatibility with standard neural layers, we decouple the geometric embeddings from subsequent computations by projecting them onto the Euclidean tangent space. For categorical features, whose category values are deterministic, the retrieved HDE projections $e^{cat} = \log_{\mathbf{0}}(\bm{Z}^{cat})$ can be used directly.
For numerical features, we need to aggregates the corresponding HDEs using $\bm{W}$ calculated in Section~\ref{sec:sdr}. Critically, to avoid expensive Möbius operations on the manifold, we project all involved LDNs to the Tangent space first, then perform weighted aggregation. Take numerical feature $j$ as an example, $\bm{Z}_j = \{z_0, ..., z_{k_j}\}, \quad \bm{Z}_j \subset \bm{Z}$. Its final Euclidean representation is:
\begin{equation}
	e_j^{num} = \frac{1}{k_j}\sum_{n=1}^{k_j}w_{j,n} \log_{\mathbf{0}}(z_{j, n}), \quad \{w_{j,1}, ..., w_{j,k_j}\} = \bm{W}_j
\end{equation}

\subsection{HDE-Net and its Hybrid Optimization Strategy}
To build the HDE-Net, the embeddings from all features $(e^{cat}, e^{num})$ are concatenated and fed into a standard, lightweight MLP predictor. To train HDE-Net, we employ a \textit{Hybrid Optimization Strategy}. The HDEs $\mathbf{Z}$ residing on the manifold are updated using Riemannian Adam \cite{becigneul2018riemannian}, which performs gradient updates in the manifold space $\mathcal{M}$. All other parameters (Routing weights $\boldsymbol{a}, \boldsymbol{b}$ and MLP weights) are updated using standard AdamW in Euclidean space. This hybrid approach ensures that the learned representations strictly adhere to hyperbolic geometry while maintaining the computational efficiency of Euclidean inference.

\section{Experiments}
\label{sec:exp}

\subsection{Setups}
\label{sec:setups}

% Table generated by Excel2LaTeX from sheet 'Sheet3'\begin{table*}[htbp]
\small  \centering  \caption{Performance of HDE-Net across all datasets in the TALENT-tiny core classification benchmark}
  \vskip -10pt
  \scalebox{0.86}{    \begin{tabular}{lccccclccccc}    \toprule    \multicolumn{12}{c}{\textbf{HDE-Net}} \\    \midrule    \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} & \multicolumn{1}{c}{\textbf{Datasets}} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{AUC} & \textbf{Accuracy} \\    \midrule    ada   & 80.90\% & 80.12\% & 80.47\% & 89.44\% & 85.60\% & law-school-admission & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% \\    airlines & 63.92\% & 63.26\% & 63.21\% & 65.05\% & 64.30\% & microaggregation2 & 57.55\% & 39.19\% & 41.22\% & 82.00\% & 63.83\% \\    allbp & 83.81\% & 64.80\% & 71.42\% & 94.95\% & 97.58\% & national-longitudinal & 99.81\% & 99.80\% & 99.81\% & 99.99\% & 99.82\% \\    ASP-POTASSCO & 37.71\% & 35.77\% & 36.03\% & 80.66\% & 42.59\% & okcupid\_stem & 67.98\% & 51.01\% & 52.81\% & 80.55\% & 75.35\% \\    autoUniv-au7-1100 & 42.35\% & 40.82\% & 40.16\% & 69.24\% & 41.95\% & online\_shoppers & 83.46\% & 77.78\% & 80.18\% & 91.79\% & 90.49\% \\    company\_bankruptcy & 85.15\% & 69.17\% & 74.33\% & 95.99\% & 97.52\% & ozone\_level & 49.11\% & 49.90\% & 49.50\% & 68.64\% & 98.03\% \\    eucalyptus & 73.34\% & 70.64\% & 71.42\% & 93.50\% & 73.92\% & pc4   & 82.58\% & 69.05\% & 73.08\% & 93.08\% & 90.72\% \\    Gender\_Gap & 53.54\% & 42.88\% & 43.90\% & 65.37\% & 60.17\% & PhishingWebsites & 97.91\% & 97.74\% & 97.82\% & 99.71\% & 97.85\% \\    hill-valley & 73.33\% & 72.81\% & 72.65\% & 77.48\% & 72.80\% & rice\_cammeo\_\&\_osmancik & 93.11\% & 92.33\% & 92.65\% & 97.62\% & 92.86\% \\    house\_16H & 88.40\% & 88.39\% & 88.39\% & 94.69\% & 88.39\% & shill-bidding & 64.99\% & 56.06\% & 56.99\% & 71.46\% & 90.24\% \\    ibm-employee-performance & 100.00\% & 100.00\% & 100.00\% & 100.00\% & 100.00\% & Shipping & 75.97\% & 73.07\% & 68.75\% & 73.77\% & 69.05\% \\    INNHotelsGroup & 87.26\% & 86.21\% & 86.69\% & 94.49\% & 88.43\% & statlog & 68.87\% & 67.75\% & 68.20\% & 73.80\% & 74.05\% \\    internet\_firewall & 93.22\% & 77.66\% & 81.90\% & 98.85\% & 93.09\% & thyroid & 96.34\% & 97.60\% & 96.92\% & 99.88\% & 99.40\% \\    jasmine & 82.10\% & 79.37\% & 78.90\% & 84.06\% & 79.35\% & waveform\_version\_1 & 87.10\% & 87.09\% & 87.06\% & 96.85\% & 87.12\% \\    jungle\_chess\_2pcs & 97.67\% & 97.68\% & 97.66\% & 99.88\% & 98.20\% & wine-quality-red & 39.23\% & 33.01\% & 33.67\% & 75.98\% & 62.97\% \\    \bottomrule    \end{tabular}%
    }  \label{tab:HDE-Net_perf}%\end{table*}%

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{overall_performance.png}
    \vskip -10pt
    \caption{Critical Difference (CD) diagram on the TALENT-tiny-core classification benchmark. Average ranks are calculated via the Wilcoxon-Holm test ($\alpha=0.05$). HDE-Net ranks 1st (4.1667), demonstrating a significant performance lead over 31 baselines.}
    \label{fig:cd_diagram}
%    \vskip -10pt
\end{figure*}

\textbf{Datasets.} 
We evaluate HDE-Net on the TALENT-tiny core classification benchmark \cite{liu2025talent}, which comprises 30 datasets (see Appendix \ref{app:datasets} for details). Unlike traditional benchmarks that are often biased toward numerical-dominant data, TALENT-tiny core provides a balanced and fair representation of various feature compositions (Numerical-only, Numerical-dominant, and Categorical-dominant). 
This diversity ensures that our evaluation fairly tests the model's ability to induce consistent symbolic structures across heterogeneous feature spaces.
Detailed statistical information for each dataset is provided in Appendix \ref{app:datasets}.

\noindent\textbf{Baselines.}
We include a comprehensive pool of 31 baselines whose results are officially disclosed on the TALENT leaderboard. These include 10 Traditional Models (e.g., CatBoost \cite{prokhorenkova2018catboost}, XGBoost \cite{chen2016xgboost}, LightGBM \cite{ke2017lightgbm}) and 21 Deep Learning Models (e.g., ModernNCA \cite{ye2024revisiting}, RealMLP \cite{holzmuller2024better}, FT-Transformer \cite{gorishniy2021revisiting}). A full list of these baselines is provided in Appendix \ref{app:comp_models}.

\noindent\textbf{Hyperparameter Settings.}
For HDE-Net, the dimension of Hyperbolic Decision Embeddings is fixed at 12 across all datasets. The number of LDNs for each numerical feature is adaptively assigned via the Entropy-Aware Capacity Allocation mechanism (Sec. \ref{sec:entropy}) with $k_{\min}=2$ and $k_{\max}=8$. To ensure a fair comparison, the hyperparameters for the MLP backbone are retrieved from the TALENT repository\footnote{\url{https://github.com/LAMDA-Tabular/TALENT}} for each specific dataset. For datasets without pre-tuned settings, we adopt the default configuration: two hidden layers of size 384 with a dropout of 0.1, trained with a learning rate of $3 \times 10^{-4}$ and weight decay of $1 \times 10^{-5}$. The LDNs are optimized using Riemannian Adam, with its learning rate and weight decay consistent with the standard AdamW optimizer used for Euclidean parameters.

\noindent\textbf{Evaluation Metrics and Ranking.}
Our model was evaluated on all datasets with 10 repeated runs, and mean accuracy was recorded. We report the average rank computed using the Wilcoxon–Holm test \cite{demvsar2006statistical} over all accuracy results at a significance level of 0.05. 

\textit{Note regarding Rank Consistency:} As ranking is a relative metric, the absolute rank of a model (e.g., HDE-Net) depends entirely on the specific model and dataset pool included in the comparison. Consequently, the ranks reported in the \textit{Performance Comparison} (Sec. \ref{sec:main_results}), \textit{Robustness across Feature Scenarios} (Sec. \ref{sec:robustness}) and the \textit{Mechanism Analysis and Ablation Study} (Sec. \ref{sec:ablation}) may differ due to the varying sets of HDE-Net variants involved.

\subsection{Performance Comparison}
\label{sec:main_results}

The overall performance of HDE-Net and 31 baseline models on the TALENT-tiny-core benchmark is shown in Table~\ref{tab:HDE-Net_perf}, and the rankings are detailed in Figure \ref{fig:cd_diagram}. HDE-Net achieves a leading Average Rank of 4.1667, securing the \textit{1st position} among all compared methods. 
HDE-Net exhibits a clear statistical advantage over a vast majority of deep learning models and traditional tree-based models. Notably, HDE-Net significantly outperforms the current state-of-the-art retrieval-based model, ModernNCA (Rank 7.63), and the industrial gold standard, XGBoost (Rank 7.98). This result validates our core design: By establishing a geometric isomorphism with decision trees at the representation level, HDE-Net effectively reconciles the discrete, rule-partitioned nature of tabular data with the differentiable learning of neural networks. It achieves superior generalization by intrinsically inducing decision-tree logic within its representation space, eliminating the need for complex backbones or computationally expensive retrieval mechanisms."

\subsection{Robustness across Feature Scenarios}
\label{sec:robustness}

To investigate the adaptability of HDE-Net to different feature compositions, we analyze the average ranking across three specific scenarios: \textit{Num-only} (Datasets contain only numerical features), \textit{Num-Heavy} (Numerical dominant datasets), and \textit{Cat-Heavy} (Categorical dominant datasets). Since the benchmark contains only one categorical-only dataset, this is merged into the \textit{Cat-Heavy} group. We select representative models from each paradigm for comparison: XGBoost (Tree), RealMLP (Ad-hoc Fusion), FT-Transformer (Pseudo-Alignment), and ModernNCA (Retrieval). The results are illustrated in Figure \ref{fig:scenarios}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{GroupedPerformance.png}
    \vskip -10pt
    \caption{Comparison of average rankings across different feature scenarios (Lower is better). HDE-Net demonstrates superior stability and performance across all scenarios, while methods like RealMLP exhibit severe specialization bias (collapsing in categorical-dominant tasks).}
    \label{fig:scenarios}
% 	\vskip -15pt
\end{figure}

\begin{itemize}
	\item \textbf{Overcoming Granularity Mismatch.} In the \textit{Num-only} scenario, HDE-Net (Rank 4.64) significantly outperforms RealMLP (Rank 7.04) and FT-Transformer (Rank 13.25). This gap empirically validates that our Soft Decision Routing effectively shifts numerical modeling from rigid feature-level linear projections to fine-grained range-based symbolic atoms. While FT-Transformer struggles with pure numerical manifolds, HDE-Net’s differentiable discretization captures the local semantics necessary for high-precision tabular reasoning.
	\item \textbf{Resolving Geometric Incompatibility.} The results highlight HDE-Net’s resilience to feature heterogeneity. Ad-hoc Fusion methods like RealMLP suffer from a specialization trap, performing decently in \textit{Num-Only} but degrading severely in \textit{Cat-Heavy} tasks (Rank 13.86). In contrast, HDE-Net maintains consistent superiority, achieving a rank of 3.57 in \textit{Cat-Heavy} scenarios, virtually tied with the tree-based champion XGBoost (3.43). This confirms that by optimizing LDNs within the Poincaré ball, HDE-Net successfully imposes a hierarchical constraint that facilitates tree-structured reasoning, bridging the gap between the categorical domain and neural optimization.
\end{itemize}

\subsection{Mechanism Analysis and Ablation Study}
\label{sec:ablation}

We conduct an ablation study to quantify the contributions of HDE-Net's components and evaluate its sensitivity to the LDN capacity $k$. \textit{Note on Rankings}: Ranks here differ from Sec.~\ref{sec:main_results} as the comparison pool includes additional HDE variants.

\noindent\textbf{Impact of Core Mechanisms} 
To disentangle the contributions of our design choices, we evaluate: \textit{1). HDE-noRouter}, reverting numerical processing to a single hyperbolic linear mapping \cite{shimizu2020hyperbolic}; \textit{2). HDE-noHyp}, retaining routing but constraining HDEs to Euclidean space. We compare these against leading baselines in Table \ref{tab:ablation}.

% Table generated by Excel2LaTeX from sheet 'Params'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Ablation study of HDE-Net components.} Results show that Soft Decision Routing alone (\textit{HDE-noHyp}) already surpasses previous SOTAs like ModernNCA, while Hyperbolic constraints (\textit{HDE-Net}) provide the critical boost to secure the top rank.}
%  \vskip -10pt
\scalebox{0.93}{
    \begin{tabular}{lrlr}
    \toprule
    \textbf{Variant} & \textbf{Avg Rank ($\downarrow$)} & \textbf{Baseline} & \textbf{Avg Rank ($\downarrow$)} \\
    \midrule
    \textbf{HDE-Net} & \textbf{4.93} & ModernNCA & 8.70 \\
    HDE-noHyp & 7.10  & TabR  & 9.48 \\
    HDE-noRouter & 13.45 & RealMLP & 10.82 \\
          &       & FT-Transformer & 13.93 \\
    \bottomrule
    \end{tabular}%
  }
  \label{tab:ablation}%
  \vskip -10pt
\end{table}%

\begin{itemize}
    \item \textbf{Soft Routing Suffices for SOTA-level Performance.} Even without hyperbolic geometry, \textit{HDE-noHyp} (Rank 7.10) outperforms previous SOTA ModernNCA (8.70). This validates that symbolic abstraction--shifting from coarse feature-level projections to fine-grained value-level nodes--is the primary driver for handling tabular heterogeneity.
    
    \item \textbf{Geometric Alignment via Hyperbolic Space.} While Euclidean routing is effective, introducing the hyperbolic manifold elevates the performance from Rank 7.10 to 4.93. This gain indicates that the Poincaré ball provides a more efficient way of modeling hierarchical decision structures. Unlike Euclidean space, which requires exponentially increasing capacity to approximate tree-like hierarchies, hyperbolic space preserves such structures more naturally, efficiently accommodating the exponential branching of decision paths.
    
    \item \textbf{High Capacity with Low Dimensionality.} Even in its most constrained form, \textit{HDE-noRouter} (Rank 13.45) remains highly competitive with the computational heavy FT-Trans\-former (13.93). It is worth noting that HDE operates with a minimal 12-dimensional embedding and a simple MLP, whereas FT-Transformer relies on high-dimensional spaces ($d>100$) and a deep Transformer backbone. This underscores the exceptional information density and efficiency of hyperbolic representations.
\end{itemize}

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{K_sensitiveness.png}
%    \vskip -10pt
    \caption{Parameter sensitivity analysis across different LDN capacities ($k$). HDE-Net (Adaptive $k$) consistently outperforms all fixed-$k$ variants, proving the necessity of entropy-aware allocation.}
%    \vskip -10pt
    \label{fig:sensitivity_bar}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{RouterVisual.png}
    \vskip -10pt
    \caption{Visualizing Soft Decision Router weights vs. Decision Tree split points. Weight intersections and zero-crossing points closely match tree thresholds, mimicking the range split feature condition of tree model.}
    \label{fig:router_vis}
%    \vskip -10pt
\end{figure*}

\noindent\textbf{Efficacy of Entropy-aware Allocation}
We analyze the parameter sensitivity of the number of LDNs ($k$). We compare HDE-Net (Adaptive $k$) against variants with fixed $k \in \{2, 4, 6, 8, 10\}$ (Fig~\ref{fig:sensitivity_bar}). Fixed-$k$ variants exhibit a clear inverted U-shape ranking trend. 
This highlights the non-uniform complexity of tabular features: Models with low capacity ($k \le 6$) fail to capture complex splitting rules, while $k=10$ introduces noise. 
Meanwhile, HDE-Net (Rank 5.73) significantly outperforms the best fixed variant ($k=8$, Rank 7.92). This trend proves that no single fixed capacity is optimal for all features, validating the necessity of breaking the rigid allocation paradigm."

\subsection{Efficiency Analysis: Complexity and Cost}
\label{sec:efficiency}

To evaluate the computational efficiency of HDE-Net, we analyze its theoretical complexity and compare it against leading baselines. 

\noindent\textbf{Theoretical Complexity.}
Let $N_{feat}$ be the number of features. Models like FT-Transformer suffer from the quadratic bottleneck of self-attention over features, leading to $\mathcal{O}(L \cdot N_{feat}^2 \cdot d)$ complexity per sample. Here $L$ is the number of layers and $d$ refers to the embedding dimension. Retrieval-based methods like ModernNCA must perform a nearest-neighbor search from the training set ($N_{train}$), resulting in $\mathcal{O}(N_{train} \cdot d)$ complexity during inference. 
In contrast, HDE-Net maintains a linear complexity. For numerical features, the Soft Decision Routing performs $K$ parallel linear operations and a weighted aggregation of $d$-dimensional HDEs. The total complexity is $\mathcal{O}(N_{feat} \cdot K \cdot d + \text{MLP})$. Crucially, HDE-Net's computation is: \textit{1). independent of training set size}; \textit{2). linear with respect to the feature count}. This allows HDE-Net to achieve superior representation power through geometric alignment while benefiting from lightweight backbone execution.

\noindent\textbf{Quantitative Comparison.}
 We selected the \texttt{INNHotelsGroup} dat\-aset from the TALENT benchmark, which contains a balanced composition of categorical (6) and numerical (11) features with 29,020 samples, providing a representative playground for different types of methods. Inference time was measured on an NVIDIA RTX A4000 GPU with a batch size of 10,240 to maximize throughput and minimize I/O overhead. We compared HDE-Net against top-performing DNN baselines: FT-Transformer (Pseudo-alignment), ModernNCA (Retrieval-based), and RealMLP (Ad-hoc Fusion), using their optimal hyperparameter configurations as provided by the benchmark. As detailed in Table \ref{tab:efficiency}, HDE-Net achieves a remarkable balance between efficiency and performance:
 
 \begin{table}[h]
  \centering
  \caption{\textbf{Efficiency Comparison on INNHotelsGroup.} Inference time is measured for a single forward pass of the full test set (Batch Size = 10,240). Rank refers to the average rank on the full TALENT benchmark (Lower is better). HDE-Net achieves the best rank with the lowest latency.}
  \vskip -10pt
  \scalebox{0.92}{
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{Params (M)} & \textbf{Infer. Time (s)} & \textbf{Avg. Rank($\downarrow$)} \\
    \midrule
    FT-Transformer & 0.91  & 0.0440 & 12.55 \\
    RealMLP & 0.15  & 0.1293$^\dagger$ & 9.68 \\
    ModernNCA & 0.06  & 0.0552 & 7.63 \\
    \textbf{HDE-Net (Ours)} & 0.63  & \textbf{0.0077} & \textbf{4.17} \\
    \bottomrule
    \multicolumn{4}{l}{\footnotesize $^\dagger$ Note: High latency likely due to implementation issue in the benchmark codebase.}
    \end{tabular}%
  \label{tab:efficiency}%
  }
\end{table}%

\begin{itemize}
    \item \textbf{Vs. Structure-based (FT-Transformer):} While HDE-Net has a comparable parameter count to FT-Transformer (0.63M vs. 0.91M), it is approximately \textit{5.7$\times$ faster} (0.0077s vs. 0.0440s). This empirically confirms that shifting complexity from the backbone (Transformer layers) to the embedding layer (HDE) significantly reduces computational latency without sacrificing accuracy.
    \item \textbf{Vs. Retrieval-based (ModernNCA):} Although ModernNCA has fewer parameters due to its non-parametric nature, its inference latency is significantly higher (0.0552s, \textit{$\approx$7.1$\times$ slower} than HDE-Net). This highlights the inherent drawback of retrieval-based inference at scale, whereas HDE-Net provides a purely parametric alternative that captures structural logic more efficiently.
    \item \textbf{Vs. Ad-hoc Fusion (RealMLP):} Theoretically, RealMLP should be efficient due to its simple architecture. However, we observed anomalously high latency (0.1293s) in the benchmark implementation. We attribute this to engineering disparities in the TALENT codebase rather than inherent algorithmic complexity. Even disregarding this anomaly, HDE-Net significantly outperforms RealMLP in ranking (4.17 vs. 9.68), justifying the overhead of the HDE layer.
\end{itemize}

In summary, HDE-Net dominates the trade-off landscape, delivering the fastest inference speed among competitive baselines while maintaining the best overall ranking.

\subsection{Qualitative Analysis: Geometric Isomorphism}
\label{sec:visualization}

To validate our core claim that HDE-Net establishes a geometric isomorphism with decision trees, we visualize the learned split logic and hierarchical structure.

\noindent\textbf{Split Logic Alignment via Router Weights.} 
We examine whether the \textit{Soft Decision Router} reconstructs the space partitioning logic of decision trees. For a specific numerical feature $j$, we visualize all related routing weights $\bm{W}_j = \{w_{j,1}, ..., w_{j, k_{j}}\}$ (from Eq.~\ref{eq:router_weight}) across its entire value range (Fig.~\ref{fig:router_vis}). The x-axis represents sorted feature input values, and each colored line corresponds to one weight within $\bm{W}_j$. 

In \texttt{ibm-employee-performance} ($k=2$) and \texttt{company-bankrup\-tcy} ($k=4$) datasets, the intersection points of these weight lines (one LDN's influence begins to surpass another) align remarkably well with the hard split thresholds learned by a standard Decision Tree (via \textit{scikit-learn}\footnote{https://scikit-learn.org/}). 
These intersections, along with the zero-crossing points (where $w_{j,k} = 0$), effectively act as differentiable decision boundaries. 
This confirms that HDE-Net implicitly reconstructs the discrete range-splitting mechanism of trees while maintaining differentiable and optimization-friendly.

\noindent\textbf{Hierarchical Structure in Poincaré Space.}
We visualize the learned distribution of categorical HDEs to confirm their hierarchical nature. We first compute the pairwise manifold distance matrix $\mathbf{D} \in \mathbb{R}^{M \times M}$ for all $M$ categorical HDEs using the Poincaré distance formula $d_{\mathbb{B}}$ defined in Eq.~\ref{eq:dis}. Then, we apply \textit{Multidimensional Scaling (MDS)} to project these hyperbolic points into a 2D Euclidean space. 
Each LDN is colored based on its \textit{Feature Importance} (Cover Rate) provided by a pre-trained XGBoost model, representing its relative level in the tree hierarchy. Note that the Soft Decision Router is not strictly equivalent to the range splits in GBDTs. Therefore, our visualization only focuses on categorical HDEs. To this end, we select two categorical-dominant datasets, \texttt{ozone\_level}\footnote{Although the dataset source reports many numerical features, the TALENT codebase treats them all as categorical, likely due to the strongly non-linear relationships between these features and the target identified by the TALENT researchers.} (36 categorical features) and \texttt{allbp} (23 categorical features), for our visualization (Fig. \ref{fig:hde_vis}). There are two findings:
\begin{figure}[h]
    \centering
    \includegraphics[width=230pt]{EmbeddingVisual.png}
    \vskip -10pt
    \caption{HDE visualized via MDS and colored by XGBoost Feature importance. The center-to-boundary hierarchy confirms the induction of tree-like structures.}
    \label{fig:hde_vis}
\end{figure}
\begin{itemize}
	\item \textbf{Branching Pattern Emergence}: Beginning with andom initialization (Epoch 0), the HDEs progressively organize into distinct clusters with fractal-like branching patterns, mirroring the topology of decision trees. 
	\item \textbf{Importance-aware Radial Distribution}: Highly important HDEs (darker red, higher cover rate) tend to reside near the center of the space, acting as root-like nodes with broader influence. Conversely, low-importance HDEs (lighter yellow) are pushed towards the boundary, representing leaf-like decision nodes. This spatial arrangement serves as direct evidence that the hyperbolic manifold constraint successfully forces the model to induce a latent reasoning hierarchy equivalent to that of GBDTs.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we have presented \textbf{HDE-Net}, a manifold-constrained framework that addresses the fundamental incompatibility between the flat Euclidean geometry of standard Tabular DNNs and the discrete, rule-partitioned nature of tabular data. By shifting the representational foundation to the hyperbolic Poincaré ball, a continuous analogue of tree structures, we successfully induce tree-structured symbolic rules within a differentiable neural architecture. 

Our core contribution lies in establishing a geometric isomorphism between neural representations and decision-tree logic. We achieve this by abstracting heterogeneous features into LDNs, where the Soft Decision Routing bridges the semantic gap by approximating local value-partitioning rules on numerical features. Furthermore, the entropy-aware allocation strategy ensures adaptive complexity management across diverse datasets. 

Extensive evaluations on the TALENT-tiny-core benchmark demonstrate that HDE-Net achieves the SOTA performance (Rank 1st), consistently outperforming industrial GBDTs and advanced Tabular DNNs. It delivers superior accuracy while maintaining significant inference speed advantages over computationally heavy transformer-based architectures or retrieval-based architectures. We believe that aligning geometric priors with the data's inherent symbolic structure represents a paradigm shift toward 'geometry-aware' tabular learning, offering a blueprint for the next generation of robust and interpretable models.

\iffalse
\section{Introduction}
ACM's consolidated article template, introduced in 2017, provides a
consistent \LaTeX\ style for use across ACM publications, and
incorporates accessibility and metadata-extraction functionality
necessary for future Digital Library endeavors. Numerous ACM and
SIG-specific \LaTeX\ templates have been examined, and their unique
features incorporated into this single new template.

If you are new to publishing with ACM, this document is a valuable
guide to the process of preparing your work for publication. If you
have published with ACM before, this document provides insight and
instruction into more recent changes to the article template.

The ``\verb|acmart|'' document class can be used to prepare articles
for any ACM publication --- conference or journal, and for any stage
of publication, from review to final ``camera-ready'' copy, to the
author's own version, with {\itshape very} few changes to the source.

\section{Template Overview}
As noted in the introduction, the ``\verb|acmart|'' document class can
be used to prepare many different kinds of documentation --- a
double-anonymous initial submission of a full-length technical paper, a
two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
journal article, a SIGCHI Extended Abstract, and more --- all by
selecting the appropriate {\itshape template style} and {\itshape
  template parameters}.

This document will explain the major features of the document
class. For further information, the {\itshape \LaTeX\ User's Guide} is
available from
\url{https://www.acm.org/publications/proceedings-template}.

\subsection{Template Styles}

The primary parameter given to the ``\verb|acmart|'' document class is
the {\itshape template style} which corresponds to the kind of publication
or SIG publishing the work. This parameter is enclosed in square
brackets and is a part of the {\verb|documentclass|} command:
\begin{verbatim}
  \documentclass[STYLE]{acmart}
\end{verbatim}

Journals use one of three template styles. All but three ACM journals
use the {\verb|acmsmall|} template style:
\begin{itemize}
\item {\texttt{acmsmall}}: The default journal template style.
\item {\texttt{acmlarge}}: Used by JOCCH and TAP.
\item {\texttt{acmtog}}: Used by TOG.
\end{itemize}

The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
\begin{itemize}
\item {\texttt{sigconf}}: The default proceedings template style.
\item{\texttt{sigchi}}: Used for SIGCHI conference articles.
\item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
\end{itemize}

\subsection{Template Parameters}

In addition to specifying the {\itshape template style} to be used in
formatting your work, there are a number of {\itshape template parameters}
which modify some part of the applied template style. A complete list
of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

Frequently-used parameters, or combinations of parameters, include:
\begin{itemize}
\item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
  conference submission. Anonymizes the work and includes line
  numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
  submission's unique ID on each page of the work.
\item{\texttt{authorversion}}: Produces a version of the work suitable
  for posting by the author.
\item{\texttt{screen}}: Produces colored hyperlinks.
\end{itemize}

This document uses the following string as the first command in the
source file:
\begin{verbatim}
\documentclass[sigconf,authordraft]{acmart}
\end{verbatim}

\section{Modifications}

Modifying the template --- including but not limited to: adjusting
margins, typeface sizes, line spacing, paragraph and list definitions,
and the use of the \verb|\vspace| command to manually adjust the
vertical spacing between elements of your work --- is not allowed.

{\bfseries Your document will be returned to you for revision if
  modifications are discovered.}

\section{Typefaces}

The ``\verb|acmart|'' document class requires the use of the
``Libertine'' typeface family. Your \TeX\ installation should include
this set of packages. Please do not substitute other typefaces. The
``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
as they will override the built-in typeface families.

\section{Title Information}

The title of your work should use capital letters appropriately -
\url{https://capitalizemytitle.com/} has useful rules for
capitalization. Use the {\verb|title|} command to define the title of
your work. If your work has a subtitle, define it with the
{\verb|subtitle|} command.  Do not insert line breaks in your title.

If your title is lengthy, you must define a short version to be used
in the page headers, to prevent overlapping text. The \verb|title|
command has a ``short title'' parameter:
\begin{verbatim}
  \title[short title]{full title}
\end{verbatim}

\section{Authors and Affiliations}

Each author must be defined separately for accurate metadata
identification.  As an exception, multiple authors may share one
affiliation. Authors' names should not be abbreviated; use full first
names wherever possible. Include authors' e-mail addresses whenever
possible.

Grouping authors' names or e-mail addresses, or providing an ``e-mail
alias,'' as shown below, is not acceptable:
\begin{verbatim}
  \author{Brooke Aster, David Mehldau}
  \email{dave,judy,steve@university.edu}
  \email{firstname.lastname@phillips.org}
\end{verbatim}

The \verb|authornote| and \verb|authornotemark| commands allow a note
to apply to multiple authors --- for example, if the first two authors
of an article contributed equally to the work.

If your author list is lengthy, you must define a shortened version of
the list of authors to be used in the page headers, to prevent
overlapping text. The following command should be placed just after
the last \verb|\author{}| definition:
\begin{verbatim}
  \renewcommand{\shortauthors}{McCartney, et al.}
\end{verbatim}
Omitting this command will force the use of a concatenated list of all
of the authors' names, which may result in overlapping text in the
page headers.

The article template's documentation, available at
\url{https://www.acm.org/publications/proceedings-template}, has a
complete explanation of these commands and tips for their effective
use.

Note that authors' addresses are mandatory for journal articles.

\section{Rights Information}

Authors of any work published by ACM will need to complete a rights
form. Depending on the kind of work, and the rights management choice
made by the author, this may be copyright transfer, permission,
license, or an OA (open access) agreement.

Regardless of the rights management choice, the author will receive a
copy of the completed rights form once it has been submitted. This
form contains \LaTeX\ commands that must be copied into the source
document. When the document source is compiled, these commands and
their parameters add formatted text to several areas of the final
document:
\begin{itemize}
\item the ``ACM Reference Format'' text on the first page.
\item the ``rights management'' text on the first page.
\item the conference information in the page header(s).
\end{itemize}

Rights information is unique to the work; if you are preparing several
works for an event, make sure to use the correct set of commands with
each of the works.

The ACM Reference Format text is required for all articles over one
page in length, and is optional for one-page articles (abstracts).

\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).

\section{Sectioning Commands}

Your work should use standard \LaTeX\ sectioning commands:
\verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
\verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
\verb|\subsusection| should be numbered; do not remove the numbering
from the commands.

Simulating a sectioning command by setting the first word or words of
a paragraph in boldface or italicized text is {\bfseries not allowed.}

Below are examples of sectioning commands.

\subsection{Subsection}
\label{sec:subsection}

This is a subsection.

\subsubsection{Subsubsection}
\label{sec:subsubsection}

This is a subsubsection.

\paragraph{Paragraph}

This is a paragraph.

\subparagraph{Subparagraph}

This is a subparagraph.

\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
high-quality tables.

Table captions are placed {\itshape above} the table.

Because tables cannot be split across pages, the best placement for
them is typically the top of the page nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and the
table caption.  The contents of the table itself must go in the
\textbf{tabular} environment, to be aligned properly in rows and
columns, with the desired horizontal and vertical rules.  Again,
detailed instructions on \textbf{tabular} material are found in the
\textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed output of
this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}

Always use midrule to separate table header rows from data rows, and
use it only for this purpose. This enables assistive technologies to
recognise table headers and support their users in navigating tables
more easily.

\section{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of the three are
discussed in the next sections.

\subsection{Inline (In-text) Equations}
A formula that appears in the running text is called an inline or
in-text formula.  It is produced by the \textbf{math} environment,
which can be invoked with the usual
\texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
and structures, from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
examples of in-text equations in context. Notice how this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols and
structures available in \LaTeX\@; this section will just give a couple
of examples of display equations in context.  First, consider the
equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or
more images can be placed within a figure. If your figure contains
third-party material, you must clearly identify it as such, as shown
in the example below.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sample-franklin}
  \caption{1907 Franklin Model D roadster. Photograph by Harris \&
    Ewing, Inc. [Public domain], via Wikimedia
    Commons. (\url{https://goo.gl/VLCRBB}).}
  \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Your figures should contain a caption which describes the figure to
the reader.

Figure captions are placed {\itshape below} the figure.

Every figure should also have a figure description unless it is purely
decorative. These descriptions convey what’s in the image to someone
who cannot see it. They are also used by search engine crawlers for
indexing images, and when images cannot be loaded.

A figure description must be unformatted plain text less than 2000
characters long (including spaces).  {\bfseries Figure descriptions
  should not repeat the figure caption – their purpose is to capture
  important information that is not already provided in the caption or
  the main text of the paper.} For figures that convey important and
complex new information, a short text description may not be
adequate. More complex alternative descriptions can be placed in an
appendix and referenced in a short figure description. For example,
provide a data table capturing the information in a bar chart, or a
structured list representing a graph.  For additional information
regarding how best to write figure descriptions and why doing this is
so important, please see
\url{https://www.acm.org/publications/taps/describing-figures/}.

\subsection{The ``Teaser Figure''}

A ``teaser figure'' is an image, or set of images in one figure, that
are placed after all author and affiliation information, and before
the body of the article, spanning the page. If you wish to have such a
figure in your article, place the command immediately before the
\verb|\maketitle| command:
\begin{verbatim}
  \begin{teaserfigure}
    \includegraphics[width=\textwidth]{sampleteaser}
    \caption{figure caption}
    \Description{figure description}
  \end{teaserfigure}
\end{verbatim}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}


  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  presentation~\cite{Reiser2014}. An article under
  review~\cite{Baggett2025}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
  Artifacts: \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

Identification of funding sources and other support, and thanks to
individuals and groups that assisted in the research and the
preparation of the work should be included in an acknowledgment
section, which is placed just before the reference section in your
document.

This section has a special environment:
\begin{verbatim}
  \begin{acks}
  ...
  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{description}
\item[\texttt{sidebar}:]  Place formatted text in the margin.
\item[\texttt{marginfigure}:] Place a figure in the margin.
\item[\texttt{margintable}:] Place a table in the margin.
\end{description}
\fi

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{unsrt}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix
\section{Statistics of the Datasets}
\label{app:datasets}
Table \ref{tab:datasets} presents detailed statistics of the datasets in the TALENT-tiny-core classification benchmark. Class denotes the number of target classes, Cat the number of categorical features, Num the number of numerical features, and Size the dataset size. All statistics are obtained from the official TALENT GitHub repository. Some details may differ from the original source of the dataset because the TALENT project has processed the dataset.
\begin{table}[htbp]
\small  \centering  \caption{Detailed statistics of the datasets in the TALENT-tiny-core classification benchmark}
  	\scalebox{1.0}{    \begin{tabular}{rrrrr}    \toprule    \multicolumn{1}{l}{\textbf{name}} & \multicolumn{1}{l}{\textbf{Class}} & \multicolumn{1}{l}{\textbf{Cat}} & \multicolumn{1}{l}{\textbf{Num}} & \multicolumn{1}{l}{\textbf{Size}} \\    \midrule    \multicolumn{1}{l}{ada} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{48} & \multicolumn{1}{c}{3317} \\    \multicolumn{1}{l}{airlines} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{1600} \\    \multicolumn{1}{l}{allbp} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{3017} \\    \multicolumn{1}{l}{ASP-POTASSCO} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{140} & \multicolumn{1}{c}{1035} \\    \multicolumn{1}{l}{autoUniv-au7-1100} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{880} \\    \multicolumn{1}{l}{company\_bankruptcy} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{93} & \multicolumn{1}{c}{5455} \\    \multicolumn{1}{l}{eucalyptus} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{14} & \multicolumn{1}{c}{588} \\    \multicolumn{1}{l}{Gender\_Gap\_in\_Spanish} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{3796} \\    \multicolumn{1}{l}{hill-valley} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{969} \\    \multicolumn{1}{l}{house\_16H} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{16} & \multicolumn{1}{c}{10790} \\    \multicolumn{1}{l}{ibm-employee-performance} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{23} & \multicolumn{1}{c}{1176} \\    \multicolumn{1}{l}{INNHotelsGroup} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{29020} \\    \multicolumn{1}{l}{internet\_firewall} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{52425} \\    \multicolumn{1}{l}{jasmine} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{136} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{2387} \\    \multicolumn{1}{l}{jungle\_chess\_2pcs} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{35855} \\    \multicolumn{1}{l}{law-school-admission} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{16640} \\    \multicolumn{1}{l}{microaggregation2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{20} & \multicolumn{1}{c}{16000} \\    \multicolumn{1}{l}{national-longitudinal} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{3926} \\    \multicolumn{1}{l}{okcupid\_stem} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{21341} \\    \multicolumn{1}{l}{online\_shoppers} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{9864} \\    \multicolumn{1}{l}{ozone\_level} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{36} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{2028} \\    \multicolumn{1}{l}{pc4} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{37} & \multicolumn{1}{c}{1166} \\    \multicolumn{1}{l}{PhishingWebsites} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{30} & \multicolumn{1}{c}{8844} \\    \multicolumn{1}{l}{rice\_cammeo\_\&\_osmancik} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{3048} \\    \multicolumn{1}{l}{shill-bidding} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5056} \\    \multicolumn{1}{l}{Shipping} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{8799} \\    \multicolumn{1}{l}{statlog} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{800} \\    \multicolumn{1}{l}{thyroid} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{5760} \\    \multicolumn{1}{l}{waveform} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{21} & \multicolumn{1}{c}{4000} \\    \multicolumn{1}{l}{wine-quality-red} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{1279} \\    \midrule          &       &       &       &  \\    \end{tabular}%
    }  \label{tab:datasets}%\end{table}%

\section{Models for Comparison}
\label{app:comp_models}
Table \ref{tab:comp_models} lists all models included in the ranking, with results provided by the official TALENT project. Note that some recent models, such as TabM and TabPFN v2, are excluded because their official benchmark results on TALENT have not yet been released, although their code has been integrated into the framework. We report full dataset-level results for HDE-Net to facilitate future comparisons.
\begin{table}[]
\caption{Models for Comparison}
\label{tab:comp_models}
\begin{tabular}{|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Traditional \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}dummy, LogReg, NCM, NaiveBayes,\\ knn, svm, xgboost, catboost,\\ RandomForest, lightgbm\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Deep Learning \\ Models\end{tabular} & \begin{tabular}[c]{@{}l@{}}tabpfn, mlp, resnet, node, switchtab,\\ tabnet, tabcaps, tangos, danets, ftt,\\ autoint, dcn2, snn, tabtransformer,\\ ptarl, grownet, tabr, modernNCA,\\ mlp\_plr, realmlp, excelformer\end{tabular} \\ \hline
\end{tabular}
\end{table}

%\section{Research Methods}
%
%\subsection{Part One}
%
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
%malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
%sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
%vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
%lacinia dolor. Integer ultricies commodo sem nec semper.
%
%\subsection{Part Two}
%
%Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
%ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
%ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
%eros. Vivamus non purus placerat, scelerisque diam eu, cursus
%ante. Etiam aliquam tortor auctor efficitur mattis.
%
%\section{Online Resources}
%
%Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
%pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
%enim maximus. Vestibulum gravida massa ut felis suscipit
%congue. Quisque mattis elit a risus ultrices commodo venenatis eget
%dui. Etiam sagittis eleifend elementum.
%
%Nam interdum magna at lectus dignissim, ac dignissim lorem
%rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
